CS50, notes
# 2진법
### 📥 Input
- 2진법
- 유일한 인풋, 전기 - 켜짐 꺼짐
- 트랜지스터 : 아주 작은 스위치 (물리적 정보 표현 값 저장) > 아주 조금의 전기를 저장해서 on하는, 나머진 끔으로 정보를 나타냄
- “정보를 표현할 때마다” 컴퓨터에서 일어나는 일
---
### 🧠 Process
2진법의 개념에 대해서는 이미 알고 있다. 우리에게 익숙한 10진법은 9에서 올림이 발생한다면, 2진법은 1에서 올림이 발생한다. 즉 한 자리에 경우의 수가 10개냐 2개냐의 차이일 뿐이다.
그렇다면 대체 왜? 컴퓨터는 2진수라는 틀에서 동작할까? 인간이 가진 개념처럼 10진수로 사용하면 안 됐나?
인체의 신비에 대해 다 알지 못 하나, 사람의 사고나 직관과 같은 것들을 해석하고 기술로 만들기란 쉽지 않나보다. 적어도 2진수 컴퓨터가 개발될 때는 그랬던 것 같다.
Ai는 어떤 방식으로 동작하는지 또 궁금해진다.
아무튼, 2진법으로 동작해야만 했던 이유에 대해 궁금해졌다. 또한 어떻게 보면 고작 2진법.. 아니 사실 10진법이라 해도 그걸 가지고 그처럼 다양한 작업들을 수행할 수 있음이 놀라울 따름이다.

왜 2진법인지에 대해 “인풋”으로 설명할 수 있을 것 같다. 모든 전자기기는 유일한 하나의 인풋을 가진다. 바로 “전기”, 우리는 전자기기를 동작하기 위해 충전을 한다.
충전이라 함은, 배터리에 전기를 채우거나 실시간으로 전기를 주는 과정이겠지. 2진법은 ‘수’의 영역이라고 하기엔 애매한 것 같다. 
2진법은 ‘수’보다는 “상태”를 나타낸다고 말하는게 더 정확할 것이다. 전기가 있냐 / 없냐가 1과 0인 것이다.
그렇기 때문에 컴퓨터는 2진법을 사용할 수밖에 없다. 10을 표현하기 위해선 전기가 꺼지는 것을 포함해 10가지 상태를 나타낼 수 있어야 하지 않은가?
굳이굳이 생각해보면 전기의 강약으로 하나의 트랜지스터에 여러 개의 값을 구현할 수 있으려나? 그러나 중요한 것은, 우리의 기술은 2진법으로 발전되어 왔고,
나는 컴퓨터를 잘 알고 활용하려는 사람이지, 새로운 컴퓨터 기술을 개발하려는 사람은 아니라는 점이다. 그렇기에 이 부분은 여기까지만.

결과적으로 컴퓨터는 “트랜지스터”라는 아주 작은 스위치를 통해 하나의 수, 혹은 bit를 가진다 할 수 있다.
이 트랜지스터가 한 컴퓨터에 무수히 많이 들어있으며, 이 트랜지스터가 켜져 있느냐(전류가 남아 있느냐) 꺼져 있느냐(전류가 남아있지 않느냐)를 통해서 정보를 나타내는 것이다.
솔직히 여전히 매우 놀랍기만 하다. 어찌보면 ‘고작?’이라고 여길 정도로 작은 트랜지스터의 전기 조합이 그러한 결과를 만들어낼 수 있다는 것이 정말로 신기하다.
앞으로 이 강의에서 그 과정을 상세하게 알아가길 기대해본다.

결국, “정보를 표현할 때마다” 컴퓨터에서는 무엇이 일어나는가? 그 정보를 표현하기 위해서 무수히 많은 트랜지스터가 onoff로 상태를 나타내는 것이다.
강의에서 123이라는 숫자를 보여주면서, 우리가 그것을 인식하는 것과 컴퓨터가 인식하는 것의 차이를 보여줬었다.
그럼 컴퓨터는 123이라는 숫자를 어떻게 화면에 띄우는 과정에까지 이르게 되는가? 123이라는 숫자를 2진수를 활용해 64+32+16+8+2+1, 즉 1111011로 나타낸다는 것 까진 알겠다. 
오케이, 123을 1111011로 저장했다는 것은 알겠다. 그럼 그걸 화면에 표시하는 과정은 또 어떻게 될까? 얼마나 치밀한 설계로 고작 “수”들의 조합으로 화면에 나타내는 명령이나,
색깔을 표현하거나 하는 등의 일이 가능할까? 이건 또 모니터 패널에 관한 영역인가? 패널을 생각해보니, 요즘 기술은 모르겠지만 옛날에 들었던 기억으로는
RGB, 즉 수많은 R, G, B 색을 내는 미세한 전구들이 배치되어 있고, 그것을 껏다 킴으로써 색 조합을 맞춰서 색을 낸다고 들었다. 
아마 그 전구는 “밝기”도 조절할 수 있을 것이다. 그러나 이 또한 자세히 파고 들면 2진법으로 밝기값을 저장하는 것이겠지?

컴퓨터를 파고드는 것은 상상 이상으로 즐거운 일이었구나.

💡 Update 1: 논리 회로에 대한 고찰

위에서 궁금한 내용에 관해서 Ai에게 질문을 했고, 몇 가지 유익한 대답을 들을 수 있었다.
전기의 강약을 통해 하나의 트랜지스터에 여러 개의 값을 구현하는 기술은 SSD에 활용되고 있다는 점이다. 신기하네.
그럼 이미 “가능”하다는 것이 밝혀졌는데 왜 컴퓨터는 2진수를 쓸까? 에 대해서도 “노이즈”라는 답변을 얻을 수 있었다.
10진법을 쓰려면 전기의 세기를 “나눠”야 하는데, 전기란 완전히 일정한 것이 아닌 출렁거리기 때문에, input과 output에 오차가 발생할 수 있다는 점이다.
그렇기 때문에 ‘정확’하게 하려면 최대한 그 변수를 통제해야 하고, 현재까지 최선의 방법이란 일정 값 이하는 전부 0, 그 이상은 전부 1로 처리하는 방식으로 정해졌다고 한다.
아마 그 “일정 값”에 근접할 일은 정말 거의 없겠지. 그 부분에 갔다면 오류가 발생하기 직전이었다는 말이니까.
갑자기 떠오른 생각은, 슈퍼마리오 시리즈 중에 한 게임의 스피드런 중 갑자기 y좌표가 심하게 바껴서 엄청난 단축이 일어났던 사건이다.
사람들은 이 글리치를 재현하고자 매우 많은 노력들로 수많은 시도를 했지만 한 번도 성공하지 못 했었다. 그러나 그 결과는 (원인은 기억이 안 나지만) 값이 변동되는… 무언가가 있었다고 했다.
오늘의 강의를 듣고, 추가적인 자료를 조사하면서 느낀 바는, 바로 이 노이즈의 오차가 “일정 값(1.5V?)”이라는 경계를 넘어버렸는데, 심지어 그것이 Y좌표에 영향을 줬던 것이라는 확신이 생겼다.
진짜 엄청난 운이었다고 밖에 말할 수가 없네..

또한, 트랜지스터 하나만 놓고 보면 단순히 onoff 스위치지만, 그룹으로 묶어 “논리 회로”를 만듦으로 AND, OR 연산 등을 수행하며 이걸 조합해서 CPU라는 계산기를 만들었다고 했다.
단순히 on/off 스위치일 때보단 뭘 조금 더 많이 할 수 있나? 싶지만 여전히 신기하고 대단하다.

예전에 회로에 대해 정말 겉핥기로 살짝 배웠었는데, 정말 여러가지가 있겠지만 대표적으로 AND, OR, XOR이 생각나네.
꼭 이게 아니더라도 on 과 off의 조합으로 무엇을 할 수 있는지를 생각해볼까.. 고작처럼 느껴지는 이 “회로”가 그 대단한 CPU를 만들어낸다니, 이건 이 강좌를 듣고 나서 생각해보지 않을수가 없었어.
…. 상당히 막막하다? and부터 하나씩 생각해보자. a와 b의 트랜지스터가 있다.. 그렇다면 and로 무엇을 표현할 수 있을까? “경우의 수”를 기준으로 생각해본다면, 2개의 트랜지스터는 4가지의 경우의 수를 만들 수 있다.
근데 그 두개를 묶어서 and를 한다면? true이거나, false이거나.. 고작 2가지다. 조합을 함으로 얻는 이득은 일단 “경우의 수”는 아니라는 것이다.
그럼 대체 and라는 연산은 무슨 이득이 있기에 사용되는건가? 컨트롤, 제어?라고 생각해볼 수 있을 것 같다. 단순히 나타낼 수 있는 경우의 수를 늘리는 것은 분명히 오류라 확신하니, 제어라 가정했을 때 어떻게 진행되는지 생각해보자.
“and는 원하는 값일 때만 1을 보낸다”라는 사실을 통해, a와 b가 둘 다 1일 때만 1을 넘기고, 그렇지 않는다면 0을 넘기게 된다. 결국 “논리 회로”란 이 과정을 더욱 효율적으로 하기 위한 수단일 것이다.
ab가 00일 때, 01일 때, 10일 때는 0이라는 값을 전달하고 싶다면, 논리회로가 없다면 3가지 경우는 0이다 라는 정보를 어딘가에 또 저장해야 될 것이고, 이것은 큰 자원 손실로 이어질 확률이 높을 것 같다.
당연히 ab가 11일 땐 1이다 라는 정보도 또한 저장해야겠지. 하지만 and회로를 설계한다면, 그러한 “저장매체” 혹은 ab를 “어떻게 처리할 것인지”에 대한 자료를 따로 저장할 필요가 없을 것이다.
이렇게 생각했을 때, 논리 회로에 대해서 이번 고찰로 내려볼 수 있는 가설이자 결론은 “논리회로가 없었어도 cpu는 똑같이 만들 수 있을 것 같다. 근데 훨씬 더 많은 트랜지스터가 필요했을 것이다”라는 점이다.

단순하게 결론 내리면 논리회로는 그냥 지름길인 것 같다. 논리회로가 없다고 못할 것은 없지만, 그 길이 너무도 오래 걸리는 방식이 될 테니까. and로만 생각해본 것이기에 다를 수도 있게지만,
모든 경우의 수에 대한 처리법을 저장한다고 한다면 결코 못할 것 같지는 않다 추측할 뿐이다.

💡 Update 2: Ai 피드백을 통한 논리회로에 대한 결론
논리 회로란 “제어”이며 스위치를 통해 ‘논리’를 구현한 것일 뿐이다. 흔히 컴퓨터가 계산을 하는 것, 수학을 하는 것처럼 보이는 것은 ‘논리’로 이루어진다.
---
### 📤 Output
**”컴퓨터는 계산기가 아니라, 논리가 구현된 ‘전기 통로’다”**
- **Why Binary? (Noise Tolerance):** 전압 차이를 이용해 더 큰 진법을 구사할 수 있겠지만, 전류는 오차가 존재하기 때문에, “신뢰성”을 최대한 지키기 위해 가장 확실한 2진법이 채택되었다.
- **What is Calculation? (Logic Gate):** 덧셈, 뺄셈 등은 적어도 컴퓨터에겐 그저 설계된 논리 게이트들이 전기 흐름을 ‘제어’한 결과일 뿐이다.
---
# 정보의 표현
### 📥 Input
- 숫자가 아닌 정보를 어떻게 표현?
- "약속"
- 'A' 는 65로 약속함
- ASCII라 하며, 8bit로 이루어짐
- 8bit로 부족해서 더 큰 단위까지 쓰는 유니코드 (상휘호환) 만듦
- 문자는 유니코드로 처리한다 하고, 사진 영상 음악 등은?
- RGB. 이미지의 1픽셀을 나타내는 기준이다
- 마찬가지로, 숫자에 따라 어떤 색을 나타낼지 '약속'함
- 72 73 33 -> R 72, G 73, B 33 > 도트 하나에 이와 같은 정보가 들어있어서 각 RGB의 정도에 따라 색이 나타남, 얼마만큼의 RGB를 각각 넣을 것인가? 를 숫자로 결정하기로 약속
- Digit의 어원은 손가락(10개)지만, CS에선 '정보를 표현하는 낱개 단위'를 뜻한다.
- 결국, 우리가 받는 모든 표현은 0과 1로 저장되어 약속에 의해 출력된 결과물일 뿐이다
---
### 🧠 Process
이번 강의에서 배운 내용은, 결국 "2진법" 강의에서 파악했던 내용들을 "실제로 어떻게 표현하는가?" 에 대한 실례를 알아본 시간이었다. 결국 컴퓨터는 0과 1밖에 할 수 없다. 그러나 이것을 통해 서로 "약속"을 하게 됨으로 우리가 보는 그 모든 과정이 이루어진다는 것이다. 우리가 서로 대화할 수 있는 것도 '언어'라는 약속이 있는 것처럼, 컴퓨터들도 어떠한 '약속'에 의해서 소통할 수 있다는 것이다. 단순히 같은 데이터(010001011010010101와 같은) 들을 주고 받을 수 있다 해도 그것을 해석할 수 없으면 고철일 뿐이지 않은가? 

그 약속은 ASCII Code부터 출발한다. 이 약속의 특징은 8bit를 사용한다는 것이다. 8bit는 10진법으로 256가지를 표현할 수 있겠지(0-255). 그러나 딱 봐도 이는 너무나 부족하다. 흔히 천자문이라 하는 한자만 담기에도 벅차며, 성조가 들어간 알파벳, 우리가 쓰는 한글 등은 약속의 방주에 탈 수가 없다. 그래서 상위호환으로 UniCode가 나왔다.
8, 16, 24bit 혹은 무려 32bit까지 정해진 약속이라는 점이다. 여기서 생긴 하나의 궁금점은 상위호환이라 했으니 ASCII의 8bit는 그대로 가져가면서, 추가적으로 약속해야 할 필요가 있는 문자들은 그 다음 수들에 배치했는가?이다. 정확히 이 내용에 대해 찾아본 적은 없지만 추측하건대, 아니 확신하건대 "맞다". 기존 약속을 완전히 깨부시고 새로운 체계를 도입했다면 굉장한 혼란이 있었을 것이며, 만약 그게 성공했다 치면, 지금 우리에게 굳이 "A"는 65다. 라는 정보를 가르칠 이유가 하등 없기 때문이다 (차라리 간략하게 이런 게 있었지만 지금은 유니코드로 바꼈다.. 정도로만 설명했을 것이다). 계속해서 A는 65, a는 97이라 가르치는 이유는 알파벳은 자주 쓰며, 유니코드로 바꼈어도 아스키의 영역은 그대로 보존되고 있었기 때문에 분명하게 동일할 것이다. 65, 97이라는 숫자는 잘 안 외워진다. 그냥 깔끔하게 16진수 41, 61이 훨씬 편하군.

Unicode에서 궁금한 건, 아니 아스키코드에서도 마찬가지로 궁금한 건.. 그래 아무튼 '약속'을 했다고 하자. 그럼 모든 데이터를 약속을 기준으로 받아들일 것인가? 72 73 33은 "Hi!"였다. 근데 이것을 이후에 나올 RGB에서는 노란색으로 하나의 픽셀을 나타낸다는 것을 배웠다. 그 차이는 어디에서 발생하는가? 컴퓨터는 0과 1을 받을 뿐이지, 이것이 Unicode 약속인지, RGB에 대한 약속인지는 알 수 없지 않는가? 그렇다면, 매 데이터를 보낼 때 이 데이터가 어떤 약속을 가지고 있는지에 대한 "헤더"가 들어갈 것이다. 예전에 배웠었던 헤더가 대충 이런 뜻이었나? 그 헤더는 아마도 데이터 묶음에서 가장 최상단에 위치할 것이며, 또 몇자리를 쓰는지도 전부 정해져있겠지? 그러면 컴퓨터는 데이터가 들어왔을 때 앞에 n자리(약속된)를 먼저 확인해서 이 데이터를 unicode로 처리할지, rgb로 처리할지 등에 대해서 결정을 하고 갈 수 있겠지. 이것은 메모장 포맷과도 연결되는 것 같다. UTF가 뭔지는 모르겠는데 메모장 저장할 때 이러한 포맷이 일치하지 않았을 때, 남이 올려놨던 파일을 내 컴퓨터에서 열 경우 다이아몬드?들이 왕창 뜨는 것을 경험했다. 어떤 약속인지 컴퓨터는 스스로 그것을 판단할 수 없기에 정해진 포맷대로 해석하고 그것을 그대로 화면에 나타내는 거지. 

사진과 영상은 결국 똑같은 개념인데, 한 장이냐 연속의 차이일 뿐이니까 묶어서 봐도 되겠다. 수많은 픽셀이 모여 있으며, 그 픽셀들 하나하나에 정보를 포함하는 것을 0과 1로 처리하는 것이다. 그럼 '음'이라는 것은 어떻게 표현될까? 굳이 지금 당장 파고들 영역은 아닌 것 같다. 음을 표현하는 방식에 대해서 0과 1로 치환한 후 그것을 동일한 약속대로 다시 음으로 풀어서 스피커로 내보내는 과정이라는 것은 확실히 이해했기 때문이다. 

💡 Update 1: 추측에 대한 답과 유니코드 처리 방식에 대한 질문

하위 호환성(Backward Compatibility): 유니코드의 앞부분은 아스키코드와 동일하다. 그래서 옛날 컴퓨터와도 대화가 통한다

맥락(Context): 0과 1 그 자체는 의미가 없으며, 이걸 해석하는 맥락(헤더, 확장자, 포맷)이 있어야 비로소 정보가 된다.

추가적으로 든 의문은 유니코드에 대해서 설명할 때, 단순히 32bit라고 하지 않고, 8, 16, 24 or 32bit라고 설명했었다. 이는 byte 단위로 1,2,3,4byte라는 것인데, 컴퓨터는 기본적으로 '최대 효율'을 중요시 여기지 않은가? 그렇다면 ascii 코드로 충분한 내용은 헤더에 8bit(혹은 1byte)라는 정보를 담아서 보낼 것으로 추측할 수 있다. 반대로 그 패킷?에서 가장 큰 코드가 24bit를 포함해야 한다면, 헤더에 24bit라는 정보를 담아서 보낼 것 같은데.. 헤더에 그 정보가 담겨있지 않는다면 데이터 중간에 0과 1이 어떤 배치로 나와도 읽어들일 수가 없을 것이니 분명 헤더(가장 앞부분)에 몇 byte인지에 대한 정보가 담겨있을 것이다. 근데 여기서 궁금해지는 건, 극단적으로 1000개의 문자가 있는데 999개는 전부 ASCII고(=8bit로 표현 가능), 나머지 1개만 32bit를 전부 써야만 표현할 수 있는 데이터라고 하면, 이 패킷은 "32bit 데이터"라는 헤더를 포함하며 999개의 8bit 데이터 앞에 0을 24개를 더 붙여버리는 비효율적인 일을 반복할까? 아니면 헤더에 '몇번째 데이터만 32bit고 나머지는 8bit다'라는 데이터를 담아서 보낼까? 후자가 훨씬 효율적일 것이다. 근데 그러면 애매하게 여러가지 크기의 데이터가 섞인 케이스는 어떻게 처리할까? 그리고 하나의 패킷의 크기는 어떻게 될까? 여러개로 나뉘어서 보낼까? 위같은 특수한 상황에 대해서 과거의 설계자들은 어떠한 방식을 채택했을까?

위 내용에 대해서 Ai와 나누기 전에 스스로 조금만 더 생각해보자. 당장 생각나는 첫 번째 방식은 "크기 별로 보내는 거"다. 8bit부터 전부 보내는데, 특정 값을 (16bit, 24bit, 32bit의 자리)라는 값을 줘서, 그 자리는 공백으로 두고, 이후에 도착하는 16bit와 24bit, 32bit 패킷에서 그 자리를 채워넣는 방식이다. 물론 이게 가능할지는 모르겠다. 두 번째 방식은 그냥 "최대bit"에 맞춰서 앞에 0을 추가해버리는 거다. 최적화에 있어서는 상당히 구릴 것 같지만 제일 확실하지 않은가? 조금의 시간만 더 쓰면 될 수도 있지. 아마 작은 용량은 그냥 이대로 처리해버릴수도 있지 않을까? 세 번째 방식은 아까 생각했었던 좌표지정이다. 근데 이건 정말 극단적인 케이스에만 쓸모 있고, 그게 아니라면 헤더가 과도하게 길어질 것 같다. 네 번째는 "쪼개기"다. 패킷을 여러개로 쪼개는데, 단순히 몇비트마다 쪼개는게 아니라, 알고리즘을 이용해서 최대bit가 잘 분배되도록 쪼개는 거다. 앞부분은 대부분 8bit라면 8bit로 끝낼 수 있는 선에서 쪼개고, 그 이후에 24bit들이 듬성듬성 있다면 그것을 묶어서 쪼개고, 또 그 다음에 8bit가 반복된다면 그것을 쪼개는 방식이다. 솔직히 적고 보니 상당히 귀찮을 것 같다. 내가 개발자가 된다면 "정말 큰 대용량 파일"이 아닌 이상 최대bit에 맞춰서 보내는게 오히려 효율적일 것 같다는 생각이 들었다.

💡 Update 2: UTF, 신호등

내가 고민한 내용에 대한 해답은 가설 2와 4가 실제로 있었던 과거 선구자들의 고민이었다는 점을 배웠다.
- UTF-32 vs UTF-8
- UTF-32 (고정길이): 나의 가설2와 같다. 무조건 가장 큰 그릇에 담는다. 처리는 빠르지만 용량 남비가 극심할 확률이 높다.
- UTF-8 (가변길이) : 나의 가설4를 크게 발전시킨 형태다. 덩어리로 쪼개는 것을 넘어서 데이터의 첫번째 비트를 "신호수"로 두는 것이다. 그래서 한 byte가 0으로 시작하면 8bit로 보고, 1로 시작하면 그것을 기점으로 16,24,32bit인지 확인 후 처리하는 방식이다. 가장 최적화된 방식 같다.

여기서 1byte = 8bit인데, ASCII는 8bit라고 배웠으니까 0으로 시작한다고 한다면 -> 1로 시작하는 128-255까지의 값은 어떻게 되는거지? 라 생각했었지만, 원래 ASCII는 7bit라는 말을 들었다. 참으로 깔끔하게 떨어지는게 신기하다. (이후 확장 ASCII가 나왔지만, unicode 약속을 위해 폐기됐다)

그럼 시작bit에 1이 들어가면 16, 24, 32bit중 하나인데, 이것을 정확히 어떤 방식으로 처리할까?
byte단위로 묶어야 된다면, 1이 들어가는 순간 최소 2byte부터 4byte중 무엇인지 판단해야 할 것이다.
그럼 첫번째 bit는 1로 고정이니까 3가지 경우의 수 (16,24,32)를 담아내려면 앞 3개의 bit를 전부 써야겠지?
100 101 110 111 이렇게 4가지를 나타낼 수 있으니 말이다. 그럼 이 중 세개는 앞으로 나올 데이터는 16,24,32bit라는 약속일테고, 나머지 하나는 그냥 쓰지 않는 bit이거나, 어떠한 다른 목적으로 쓴다거나, 그 값이 나온다면 "데이터에 손상이 갔다"라는 신호가 될 수도 있겠네 (나올 수 없는 값일테니).

아무튼 그럼 3bit를 신호등으로 할당해야 되는데, 그럼 16bit는 사실상 13bit만, 24bit는 21bit만, 32bit는 29bit만 사용해서 다 표현하도록 했나? (앞의 3개의 bit가 변동되는 값은 약속하지 않은거지) 아마 그렇겠지

💡 Update 3: 놀라운 안전장치

내가 생각했던 방식과 유사하지만, bit수를 조금 더 희생함으로 "신뢰성"을 크게 올리는 방식을 채택했다.
나는 앞의 3bit를 사용한다 생각했지만, 실제로는 1이 몇 번 반복됐는가? (그 마침표는 0이다)로 설계됐다.
- 1byte : 0XXX XXXX
- 2byte : 110X XXXX 10XX XXXX
- 3byte : 1110 XXXX 10XX XXXX 10XX XXXX
- 4byte : 1111 0XXX 10XX XXXX 10XX XXXX 10XX XXXX

한 byte를 해석할 때 0부터 나오면 1byte로 처리한다.
1부터 나오면 1이 몇 번 나왔는지를 체크한다. 2byte는 110으로 시작한다. 1byte를 제외하고 최소 단위임에도 10이 아닌 이유는 연속되는 byte들에서 10으로 해당 byte가 열차에 달린 몸통임을 확인해줘야 했기 때문이다.
즉, 출발 열차에서 몇 byte인지 1의 개수로 판단한 다음, 다음 byte가 10으로 시작되면 연속된 열차임을 검증할 수 있다. 그리하여 머리에서 선언된 열차길이와 몸통(10으로 시작하는 byte)의 개수가 일치하지 않는다면, 컴퓨터는 그것이 잘못됐음을 검사하고 "대체 문자(Replacement Character)"로 표시해준 후에 넘어갈 수 있는거다.

그럼 각 byte는 얼마만큼의 bit를 희생할까?
[ 1byte - 1bit / 2byte - 5bit / 3byte 8bit / 4byte 11bit ]
굉장히 많이 희생하는 것 같지만 여전히 남은 자리는 많을 것 같다. 32bit에서 11bit를 희생해도 21bit인데, 이게 얼마나 큰 수인가? 모든 나라의 문자를 표현하고 이모티콘 등을 넣어도 부족함이 없어보인다.
--- 
### 📤 Output
**"No 약속, No 정보"**
---
# 알고리즘
### 📥 Input
- 문제 해결을 위한 단계적 방법일 뿐
- 대부분의 경우, 우리가 당연히 하는 직관이나 생각들을 기계가 이해하도록 번역하는 것
- 그럼 뭐가 좋은 알고리즘인가?
- 의사 코드(pseudo code)
--- 
### 🧠 Process
알고리즘은 참으로 많이 들어봤다. 이 단어는 단순히 컴퓨터에서 쓰이는 것을 넘어서, 요즘 아이들은 "유튜브" 혹은 "인스타그램" 등에 더 연관지어서 생각할 것이다. 결국 그 또한 '알고리즘'이겠지.
알고리즘이란, '처리'라고 생각하면 될 것 같다. 강의에서도 Input > [ Black Box ] > Output을 초반에 보여주고, 지금 와서 "이것은 알고리즘이었다"고 설명한다.
나는 앞서서 컴퓨터는 0과 1로 이루어져 있을 뿐임을 알게 되었다. 그렇다면, 이 "처리과정"인 알고리즘, 단순하게 생각하면 "논리의 흐름"이라고도 볼 수 있는 이 알고리즘은 0과 1에서 어떠한 영향을 끼치게 될 것인가? 

위 내용에 대한 설명을 하긴 어려울 것 같다. 강의에서 설명한 알고리즘은, 그냥 "문제 해결 방식 매뉴얼"이었지만, 내가 설명하고자 하는 것은 그 메뉴얼을 0과 1이 어떻게 처리하는지?에 대한 고민이기 때문이다. 교수가 이번 강의에서 소개한 알고리즘은 2가지다. 하나는 순차적으로 페이지를 확인하며 찾는 방법, 다른 하나는 절반씩 쪼개가는 방법이다.

천 명의 이름이 담긴 책에서 "Sky Ha"라는 이름을 찾기 위해서 1페이지부터 하나씩 본다고 가정했을 때, 그것을 0과 1로 대체 어떻게 구현할 수 있을까? 일단 여기에서 한 가지 전제가 있었다. "가나다 순"으로 이미 깔끔하게 정렬되어 있었다는 것이다. 이 정렬 또한 알고리즘의 힘이겠지만, 지금 당장은 이 부분은 해결되어 있다고 치자. "Sky Ha"라는 데이터를 분석해보면, unicode를 통해 S, k, y, ' ', H, a 로 되어 있겠지. 이 책은 이미 가나다 순으로 정렬이 돼있으니까, 가장 앞에 S만 보면 될 거다. S는 0x53이며, 컴퓨터가 맨 앞에서부터 "이름"이라는 단위를 하나씩 검사할 거다. 가장 앞에 코드가 0x53보다 작으면, 다음으로 넘기고 같으면 'k'를 검증하겠지. 그리고 또 0x53으로 시작하는 모든 값을 뒤졌음에도 'Sky Ha'가 발견되지 않았다면, 책에 없음을 파악하고 종료하는 정도의 안전장치는 둘 수 있을 것이다. 그럼 대체 어떻게 0x53인지 검증할까? 우리가 보기엔 그냥 if(첫글자 = 0x53) 정도로 간단하게 생각하겠지만, 나는 0과 1이 이 일을 어떻게 수행하는지가 궁금하다.

아마 이것을 제어하기 위해선, '논리회로'가 필요하지 않을까?
0x53을 2진수로 표현하면, 0101 0011 이다. 그럼 컴퓨터는 첫 데이터를 읽고, 0101 0011을 tmp와 같은 곳에 저장해둔 다음에, 각 bit에 대해서 xor 연산을 진행하겠지? (and나 or은 절대 안 되고, 아무튼 두 값이 일치하냐 / 다르냐를 판단할 수 있는 회로를 쓸 것이 분명하다. 그게 xor이 맞는지는 잘 모르겠네). 그럼 또 '주소'도 필요하겠네. 책의 첫 이름부터 끝 이름까지 주소가 있을테고, 그것 또한 가지고 있으면서, 한 이름이 찾는 이름이 아닐 때 주소값을 +1 하는 과정이 추가되겠지, 그럼 순서 상으로는 정답이 아닐 땐 n번 주소 xor 연산 > n++ 을 반복하게 될 거다. 
아마 이렇게 돌아갈 수밖에 없을 것이다. 근데 이렇게 '돌아가는 게' 나는 신기하다. 지금 이것을 고민해서 답이 나올까? 0과 1을 다루는 것에 있어서 "이해"는 했지만, 실제로 그것이 동작하는 과정이 상당히 궁금하게 느껴지는 하루다.

결국 컴퓨터는 하등 존재이며, 사람은 고등 존재인 것 같다. 아무리 컴퓨터가 좋은 기술로 만들어지고 놀라운 회로들의 조합이라 하여도, 사람이 훨씬 차원이 높은 존재다. 생명의 탄생의 신비부터, 성장에 필요한 것들이 주어지며 사람은 커간다. 그리하여 사람에게는 '당연히 할 수 있는 것들'이 매우 많다. 주소록에서 찾는 거? 컴퓨터처럼 열심히 코딩을 안 해도 적당히 머리가 큰 사람이라면 알아서 'S는 어딨냐..'부터 시작해서 찾아내겠지. 알고리즘은 이렇게 사람이 당연히 할 수 있는 것들을 아주 세세하게 쪼개서 지시하는 것이라 할 수 있겠다. 그러나 때로는 0과 1로 이루어진 무수한 트랜지스터들이.. 마치 개미와 같이 하나하나의 힘은 매우 약할지언정 사람에겐 사실상 불가능한, 혹은 사람보다 훨씬 빠른 속도로 수많은 일들을 처리할 수 있다는 것 또한 사실일 것이다.

이러한 알고리즘에서 무엇이 좋은지는 딱 2가지 측면으로 정리되지 않을까? 하나는 "신뢰성"이며, "효율성"일 것 같다. 결과가 신뢰도가 있어야 하며, 탐색하는데 걸리는 자원(시간, 혹은 메모리 할당량 등)이 적을 수록 좋다. 당장 생각해본다면, 신뢰도가 무조건 100%여야 하지 않나? 라고 생각이 들었지만, 그것 어디까지나 case by case 일 것 같다. 신뢰도를 조금 포기하고 엄청난 자원 절약을 할 수 있다면, 그것이 무조건 나쁘다고만은 할 수 없을 것 같다. 예를 들어 "책에 무조건 찾고자 하는 이름이 있다!" 라는 전제조건이 붙어있는 상황에서 "확실하게 한 번 만에 찾는 100시간 짜리 알고리즘" vs "10%로 찾을 수 있지만 10초가 걸리는 알고리즘" 이 있다면, 그냥 후자를 찾을 때까지 반복하는게 훨씬 이득이겠지.
아무튼 모든 상황에서 신뢰도를 100% 기준치로 잡고 생각할 필요는 없는 것 같다.

이러한 알고리즘을 0과 1로 풀어서 설명할 수 있겠지만, 그것은 사람에게 적합하지 않다. 사람은 차원이 다른 '직관'을 가졌기 때문에, "흐름"만 설명해도 이해할 수 있다. 컴퓨터는 그것을 이해하지 못 하지만 말이다. 그래서 사용하는게 "의사코드"다. 이것은 컴퓨터에게 알려주기 위해서 만드는게 아닌, 사람 입장에서 이해를 돕기 위한 작업이며, 사람은 의사코드를 통해 알고리즘을 이해하고, 이것을 컴퓨터에게 수준을 낮춰서 설명해줘야만 작업이 완료됐다고 할 수 있다. 컴퓨터가 어떻게 동작하는지 깊게 이해하며 탐구하는 것도 중요하지만, 의사코드를 잘 활용해서 사람인 내가 훨씬 빠르게 논리를 구현하는 것 또한 열심히 연습해야겠다.

💡 Update 1: null, 주소 +1의 숨은 의미, Big O Notation

문자열의 끝에는 무조건 \0 (Null Terminator)가 숨겨져 있다고 한다. 왜냐하면 메모리란, 빈집을 주는게 아니라 전 세입자가 있었던 상태며, Sky Ha가 문자열로 들어와 있다 해도, 컴퓨터가 읽을 때 'a' 뒤의 문자까지 읽어버리기 때문에, 그것을 끝낸다는 약속으로 0x00인 null을 넣는다. 만약 이 약속이 null이 아닌 K였다 치면 컴퓨터는 데이터를 이렇게 읽겠지. Sky HaKSky HbcKSli.... 그럼 구분하기 힘들테니... Null의 소중함을 다시 한 번 기억하게 됐다. 

또, 주소값을 +1하는 과정은 '포인터 연산'과 관련이 있다고 한다. 그러나 이 +1이 무조건 1byte가 아님을 기억하고 있어야 되는 것 같다. Sky Ha만 봐도 null을 포함해 S, k, y, ' ', H, a, \0 총 7개의 byte를 사용하고 있지 않은가? 만약 +1을 단순하게 해버린다면 BlueSky Ha라는 이름을 컴퓨터는 "옳다 요놈이구나!" 해버릴 듯 싶다. (B > l > u > e까지는 틀렸다고 여기다가 S부터 찾아버리니까 말이다). 이렇게 보니 Ctrl + F 기능을 사용해서 "포함된"을 찾을 때는, 주소를 +1 Byte로 찾을 것 같다고 생각이 든다. BlueSky Ha라는 단어가 있다면, 이 또한 'Sky Ha'를 찾을 때 검색해야 되니 말이다. 반대로 정확히 일치한 것을 찾을 때는 무조건 첫 글자부터 확인하고 아니면 바로 다음 byte가 아닌 다음 "이름"으로 넘어가는 과정이 있겠지. 그래서 "데이터 타입"을 알고 있어야 주소를 얼마나 점프할 지 계산할 수 있다고 했다.

음.. 근데, data 타입이라는 정보에 Sky Ha가 총 7개의 byte라는 정보가 있을까? 많은 사람들의 이름이 기록되어 있고 글자 수는, 즉 값의 크기는 엄청 다양할 것이다. 근데 data type으로 판단할 수가 있나? int일 때 4byte를 점프한다, char일 때 1byte를 점프한다 까지는 알겠는데, 이름이 다 다른데 어떻게 정확히 다음 이름으로 점프할 수 있을까? 제일 편한 방법은 최대 글자 수에 맞춰 모든 이름을 그만큼의 크기로 할당해버리면 되겠지. 그럼 점프할 때 그냥 n byte씩 점프하면 그만일테지. 근데 그런 비효율을 썼을리는 없을거다. 아마 '가변 string?' 같은게 들어가서 몇 byte를 점프해야 되는지 쉽게 찾아낼 수 있겠지? 그 원리에 대해서 지금 고민하는 건 조금 과한 것 같다. 나중에 data type에서 배우고 고민해보자.

결과적으로 그냥 이름 길이를 정해버리는 방식과, 자유롭게 주소로 쓰는 방식을 둘 다 쓴다는 것을 알게 됐다. 전자는 고민할 것도 없고, 후자는 "주소"를 적극 활용한다는 것을 알게 됐다. 주소값은 모두 동일한 크기를 가진다. "점프"는 이름이 아닌 주소를 점프하는 거다. A라는 이름을 보고 아니네? 그럼 A의 위치에서 점프하는게 아니라 A의 주소를 점프하면, 무조건 B의 주소가 나오는거다. 그럼 B의 주소를 찾아가는 방식이다. 이런 식이라면, 데이터의 양에 따라 결정할 수 있겠다. 한국으로 치면 이름이 대부분 3글자다. 아무리 커도 4글자라 쳤을 때, 4글자는 소수니까 특수케이스로 분류하고, 3글자 고정으로 쓰면 "주소"를 할당할 자원을 아낄 수 있다. 2글자 이름은 한 자리 조금 버려도 사소하다. 반대로 엄청 들쭉날쭉한 영어 이름같은 경우, 차라리 주소를 할당하는게 훨씬 자원을 아낄 것이다. 두 방식을 결정하는데 유일한 시금석은 '효율'이겠지.

알고리즘의 효율성을 "값"으로 나타내는 방법이 있다고 한다. 바로 Big O다. 대충 O(x)와 같은 방식으로 쓰는 것 같다. x에는 "최악의 경우의 수"를 적어야 하는 것 같다. '이 알고리즘에서 가장 오래 걸렸을 때가 몇 번인가?'를 적으면 O(x)가 얼마나 뛰어난 알고리즘인지 비교할 수 있는 표기법이다. 순차 탐색은 N명의 사람을 찾을 때 운 없으면 가장 끝자리, 즉 N번이니까 O(N)이다. 만약 바보같이 한 이름을 두 번씩 보는 알고리즘이면 O(2N)이겠고, 강의에서 나왔던 것처럼 2명씩 넘기는 알고리즘은 운이 없어서 2명씩 넘기는 그 사이에 걸려서 "어? Sky Ha를 넘어갔네?"라고 보고 다시 앞으로 한 단계를 간다 가정해도 O(1/2N+1)정도겠지. 아마 +1은 정말 사소하니까 생략하지 않을까 싶다. 그리고 이진 탐색은 O(log N)이었다. 강의시간에 표로 보여준 내용이었는데, 빅 오로 이렇게 표현하구나. 아무튼 x값을 봄으로 어떤 알고리즘이 "더 우월한가"에 대해 쉽게 판단할 수 있는 표기법이라는 것을 알게 되었다.
더 알아보니 상수나 계수는 그냥 버린다고 한다. +1 뿐만 아니라 2N도 그냥 N이랑 비슷하다 이거다. 알고리즘은 "정말 많은 데이터"를 처리하는 것을 기준으로 보기 때문에, 2배 차이는 결국 2배밖에 안 난다는 거다. 단순히 log N과 비교해봐도 엄청난 차이가 나니 납득할만하다.
---
### 📤 Output
**개발자는 사장이다. 직원들을 어떻게 일 시키느냐에 따라 회사의 가치가 결정된다**
