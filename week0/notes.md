CS50, notes
# 2진법
### 📥 Input
- 2진법
- 유일한 인풋, 전기 - 켜짐 꺼짐
-트랜지스터 : 아주 작은 스위치 (물리적 정보 표현 값 저장) > 아주 조금의 전기를 저장해서 on하는, 나머진 끔으로 정보를 나타냄
- “정보를 표현할 때마다” 컴퓨터에서 일어나는 일
---
### 🧠 Process
2진법의 개념에 대해서는 이미 알고 있다. 우리에게 익숙한 10진법은 9에서 올림이 발생한다면, 2진법은 1에서 올림이 발생한다. 즉 한 자리에 경우의 수가 10개냐 2개냐의 차이일 뿐이다.
그렇다면 대체 왜? 컴퓨터는 2진수라는 틀에서 동작할까? 인간이 가진 개념처럼 10진수로 사용하면 안 됐나?
인체의 신비에 대해 다 알지 못 하나, 사람의 사고나 직관과 같은 것들을 해석하고 기술로 만들기란 쉽지 않나보다. 적어도 2진수 컴퓨터가 개발될 때는 그랬던 것 같다.
Ai는 어떤 방식으로 동작하는지 또 궁금해진다.
아무튼, 2진법으로 동작해야만 했던 이유에 대해 궁금해졌다. 또한 어떻게 보면 고작 2진법.. 아니 사실 10진법이라 해도 그걸 가지고 그처럼 다양한 작업들을 수행할 수 있음이 놀라울 따름이다.

왜 2진법인지에 대해 “인풋”으로 설명할 수 있을 것 같다. 모든 전자기기는 유일한 하나의 인풋을 가진다. 바로 “전기”, 우리는 전자기기를 동작하기 위해 충전을 한다.
충전이라 함은, 배터리에 전기를 채우거나 실시간으로 전기를 주는 과정이겠지. 2진법은 ‘수’의 영역이라고 하기엔 애매한 것 같다. 
2진법은 ‘수’보다는 “상태”를 나타낸다고 말하는게 더 정확할 것이다. 전기가 있냐 / 없냐가 1과 0인 것이다.
그렇기 때문에 컴퓨터는 2진법을 사용할 수밖에 없다. 10을 표현하기 위해선 전기가 꺼지는 것을 포함해 10가지 상태를 나타낼 수 있어야 하지 않은가?
굳이굳이 생각해보면 전기의 강약으로 하나의 트랜지스터에 여러 개의 값을 구현할 수 있으려나? 그러나 중요한 것은, 우리의 기술은 2진법으로 발전되어 왔고,
나는 컴퓨터를 잘 알고 활용하려는 사람이지, 새로운 컴퓨터 기술을 개발하려는 사람은 아니라는 점이다. 그렇기에 이 부분은 여기까지만.

결과적으로 컴퓨터는 “트랜지스터”라는 아주 작은 스위치를 통해 하나의 수, 혹은 bit를 가진다 할 수 있다.
이 트랜지스터가 한 컴퓨터에 무수히 많이 들어있으며, 이 트랜지스터가 켜져 있느냐(전류가 남아 있느냐) 꺼져 있느냐(전류가 남아있지 않느냐)를 통해서 정보를 나타내는 것이다.
솔직히 여전히 매우 놀랍기만 하다. 어찌보면 ‘고작?’이라고 여길 정도로 작은 트랜지스터의 전기 조합이 그러한 결과를 만들어낼 수 있다는 것이 정말로 신기하다.
앞으로 이 강의에서 그 과정을 상세하게 알아가길 기대해본다.

결국, “정보를 표현할 때마다” 컴퓨터에서는 무엇이 일어나는가? 그 정보를 표현하기 위해서 무수히 많은 트랜지스터가 onoff로 상태를 나타내는 것이다.
강의에서 123이라는 숫자를 보여주면서, 우리가 그것을 인식하는 것과 컴퓨터가 인식하는 것의 차이를 보여줬었다.
그럼 컴퓨터는 123이라는 숫자를 어떻게 화면에 띄우는 과정에까지 이르게 되는가? 123이라는 숫자를 2진수를 활용해 64+32+16+8+2+1, 즉 1111011로 나타낸다는 것 까진 알겠다. 
오케이, 123을 1111011로 저장했다는 것은 알겠다. 그럼 그걸 화면에 표시하는 과정은 또 어떻게 될까? 얼마나 치밀한 설계로 고작 “수”들의 조합으로 화면에 나타내는 명령이나,
색깔을 표현하거나 하는 등의 일이 가능할까? 이건 또 모니터 패널에 관한 영역인가? 패널을 생각해보니, 요즘 기술은 모르겠지만 옛날에 들었던 기억으로는
RGB, 즉 수많은 R, G, B 색을 내는 미세한 전구들이 배치되어 있고, 그것을 껏다 킴으로써 색 조합을 맞춰서 색을 낸다고 들었다. 
아마 그 전구는 “밝기”도 조절할 수 있을 것이다. 그러나 이 또한 자세히 파고 들면 2진법으로 밝기값을 저장하는 것이겠지?

컴퓨터를 파고드는 것은 상상 이상으로 즐거운 일이었구나.

💡 Update 1: 논리 회로에 대한 고찰

위에서 궁금한 내용에 관해서 Ai에게 질문을 했고, 몇 가지 유익한 대답을 들을 수 있었다.
전기의 강약을 통해 하나의 트랜지스터에 여러 개의 값을 구현하는 기술은 SSD에 활용되고 있다는 점이다. 신기하네.
그럼 이미 “가능”하다는 것이 밝혀졌는데 왜 컴퓨터는 2진수를 쓸까? 에 대해서도 “노이즈”라는 답변을 얻을 수 있었다.
10진법을 쓰려면 전기의 세기를 “나눠”야 하는데, 전기란 완전히 일정한 것이 아닌 출렁거리기 때문에, input과 output에 오차가 발생할 수 있다는 점이다.
그렇기 때문에 ‘정확’하게 하려면 최대한 그 변수를 통제해야 하고, 현재까지 최선의 방법이란 일정 값 이하는 전부 0, 그 이상은 전부 1로 처리하는 방식으로 정해졌다고 한다.
아마 그 “일정 값”에 근접할 일은 정말 거의 없겠지. 그 부분에 갔다면 오류가 발생하기 직전이었다는 말이니까.
갑자기 떠오른 생각은, 슈퍼마리오 시리즈 중에 한 게임의 스피드런 중 갑자기 y좌표가 심하게 바껴서 엄청난 단축이 일어났던 사건이다.
사람들은 이 글리치를 재현하고자 매우 많은 노력들로 수많은 시도를 했지만 한 번도 성공하지 못 했었다. 그러나 그 결과는 (원인은 기억이 안 나지만) 값이 변동되는… 무언가가 있었다고 했다.
오늘의 강의를 듣고, 추가적인 자료를 조사하면서 느낀 바는, 바로 이 노이즈의 오차가 “일정 값(1.5V?)”이라는 경계를 넘어버렸는데, 심지어 그것이 Y좌표에 영향을 줬던 것이라는 확신이 생겼다.
진짜 엄청난 운이었다고 밖에 말할 수가 없네..

또한, 트랜지스터 하나만 놓고 보면 단순히 onoff 스위치지만, 그룹으로 묶어 “논리 회로”를 만듦으로 AND, OR 연산 등을 수행하며 이걸 조합해서 CPU라는 계산기를 만들었다고 했다.
단순히 on/off 스위치일 때보단 뭘 조금 더 많이 할 수 있나? 싶지만 여전히 신기하고 대단하다.

예전에 회로에 대해 정말 겉핥기로 살짝 배웠었는데, 정말 여러가지가 있겠지만 대표적으로 AND, OR, XOR이 생각나네.
꼭 이게 아니더라도 on 과 off의 조합으로 무엇을 할 수 있는지를 생각해볼까.. 고작처럼 느껴지는 이 “회로”가 그 대단한 CPU를 만들어낸다니, 이건 이 강좌를 듣고 나서 생각해보지 않을수가 없었어.
…. 상당히 막막하다? and부터 하나씩 생각해보자. a와 b의 트랜지스터가 있다.. 그렇다면 and로 무엇을 표현할 수 있을까? “경우의 수”를 기준으로 생각해본다면, 2개의 트랜지스터는 4가지의 경우의 수를 만들 수 있다.
근데 그 두개를 묶어서 and를 한다면? true이거나, false이거나.. 고작 2가지다. 조합을 함으로 얻는 이득은 일단 “경우의 수”는 아니라는 것이다.
그럼 대체 and라는 연산은 무슨 이득이 있기에 사용되는건가? 컨트롤, 제어?라고 생각해볼 수 있을 것 같다. 단순히 나타낼 수 있는 경우의 수를 늘리는 것은 분명히 오류라 확신하니, 제어라 가정했을 때 어떻게 진행되는지 생각해보자.
“and는 원하는 값일 때만 1을 보낸다”라는 사실을 통해, a와 b가 둘 다 1일 때만 1을 넘기고, 그렇지 않는다면 0을 넘기게 된다. 결국 “논리 회로”란 이 과정을 더욱 효율적으로 하기 위한 수단일 것이다.
ab가 00일 때, 01일 때, 10일 때는 0이라는 값을 전달하고 싶다면, 논리회로가 없다면 3가지 경우는 0이다 라는 정보를 어딘가에 또 저장해야 될 것이고, 이것은 큰 자원 손실로 이어질 확률이 높을 것 같다.
당연히 ab가 11일 땐 1이다 라는 정보도 또한 저장해야겠지. 하지만 and회로를 설계한다면, 그러한 “저장매체” 혹은 ab를 “어떻게 처리할 것인지”에 대한 자료를 따로 저장할 필요가 없을 것이다.
이렇게 생각했을 때, 논리 회로에 대해서 이번 고찰로 내려볼 수 있는 가설이자 결론은 “논리회로가 없었어도 cpu는 똑같이 만들 수 있을 것 같다. 근데 훨씬 더 많은 트랜지스터가 필요했을 것이다”라는 점이다.

단순하게 결론 내리면 논리회로는 그냥 지름길인 것 같다. 논리회로가 없다고 못할 것은 없지만, 그 길이 너무도 오래 걸리는 방식이 될 테니까. and로만 생각해본 것이기에 다를 수도 있게지만,
모든 경우의 수에 대한 처리법을 저장한다고 한다면 결코 못할 것 같지는 않다 추측할 뿐이다.

💡 Update 2: Ai 피드백을 통한 논리회로에 대한 결론
논리 회로란 “제어”이며 스위치를 통해 ‘논리’를 구현한 것일 뿐이다. 흔히 컴퓨터가 계산을 하는 것, 수학을 하는 것처럼 보이는 것은 ‘논리’로 이루어진다.
---
### 📤 Output
**”컴퓨터는 계산기가 아니라, 논리가 구현된 ‘전기 통로’다”**
- **Why Binary? (Noise Tolerance):** 전압 차이를 이용해 더 큰 진법을 구사할 수 있겠지만, 전류는 오차가 존재하기 때문에, “신뢰성”을 최대한 지키기 위해 가장 확실한 2진법이 채택되었다.
- **What is Calculation? (Logic Gate):** 덧셈, 뺄셈 등은 적어도 컴퓨터에겐 그저 설계된 논리 게이트들이 전기 흐름을 ‘제어’한 결과일 뿐이다.
---
# 정보의 표현
### 📥 Input
- 숫자가 아닌 정보를 어떻게 표현?
- "약속"
- 'A' 는 65로 약속함
- ASCII라 하며, 8bit로 이루어짐
- 8bit로 부족해서 더 큰 단위까지 쓰는 유니코드 (상휘호환) 만듦
- 문자는 유니코드로 처리한다 하고, 사진 영상 음악 등은?
- RGB. 이미지의 1픽셀을 나타내는 기준이다
- 마찬가지로, 숫자에 따라 어떤 색을 나타낼지 '약속'함
- 72 73 33 -> R 72, G 73, B 33 > 도트 하나에 이와 같은 정보가 들어있어서 각 RGB의 정도에 따라 색이 나타남, 얼마만큼의 RGB를 각각 넣을 것인가? 를 숫자로 결정하기로 약속
- Digit의 어원은 손가락(10개)지만, CS에선 '정보를 표현하는 낱개 단위'를 뜻한다.
- 결국, 우리가 받는 모든 표현은 0과 1로 저장되어 약속에 의해 출력된 결과물일 뿐이다
---
### 🧠 Process
이번 강의에서 배운 내용은, 결국 "2진법" 강의에서 파악했던 내용들을 "실제로 어떻게 표현하는가?" 에 대한 실례를 알아본 시간이었다. 결국 컴퓨터는 0과 1밖에 할 수 없다. 그러나 이것을 통해 서로 "약속"을 하게 됨으로 우리가 보는 그 모든 과정이 이루어진다는 것이다. 우리가 서로 대화할 수 있는 것도 '언어'라는 약속이 있는 것처럼, 컴퓨터들도 어떠한 '약속'에 의해서 소통할 수 있다는 것이다. 단순히 같은 데이터(010001011010010101와 같은) 들을 주고 받을 수 있다 해도 그것을 해석할 수 없으면 고철일 뿐이지 않은가? 

그 약속은 ASCII Code부터 출발한다. 이 약속의 특징은 8bit를 사용한다는 것이다. 8bit는 10진법으로 256가지를 표현할 수 있겠지(0-255). 그러나 딱 봐도 이는 너무나 부족하다. 흔히 천자문이라 하는 한자만 담기에도 벅차며, 성조가 들어간 알파벳, 우리가 쓰는 한글 등은 약속의 방주에 탈 수가 없다. 그래서 상위호환으로 UniCode가 나왔다.
8, 16, 24bit 혹은 무려 32bit까지 정해진 약속이라는 점이다. 여기서 생긴 하나의 궁금점은 상위호환이라 했으니 ASCII의 8bit는 그대로 가져가면서, 추가적으로 약속해야 할 필요가 있는 문자들은 그 다음 수들에 배치했는가?이다. 정확히 이 내용에 대해 찾아본 적은 없지만 추측하건대, 아니 확신하건대 "맞다". 기존 약속을 완전히 깨부시고 새로운 체계를 도입했다면 굉장한 혼란이 있었을 것이며, 만약 그게 성공했다 치면, 지금 우리에게 굳이 "A"는 65다. 라는 정보를 가르칠 이유가 하등 없기 때문이다 (차라리 간략하게 이런 게 있었지만 지금은 유니코드로 바꼈다.. 정도로만 설명했을 것이다). 계속해서 A는 65, a는 97이라 가르치는 이유는 알파벳은 자주 쓰며, 유니코드로 바꼈어도 아스키의 영역은 그대로 보존되고 있었기 때문에 분명하게 동일할 것이다. 65, 97이라는 숫자는 잘 안 외워진다. 그냥 깔끔하게 16진수 41, 61이 훨씬 편하군.

Unicode에서 궁금한 건, 아니 아스키코드에서도 마찬가지로 궁금한 건.. 그래 아무튼 '약속'을 했다고 하자. 그럼 모든 데이터를 약속을 기준으로 받아들일 것인가? 72 73 33은 "Hi!"였다. 근데 이것을 이후에 나올 RGB에서는 노란색으로 하나의 픽셀을 나타낸다는 것을 배웠다. 그 차이는 어디에서 발생하는가? 컴퓨터는 0과 1을 받을 뿐이지, 이것이 Unicode 약속인지, RGB에 대한 약속인지는 알 수 없지 않는가? 그렇다면, 매 데이터를 보낼 때 이 데이터가 어떤 약속을 가지고 있는지에 대한 "헤더"가 들어갈 것이다. 예전에 배웠었던 헤더가 대충 이런 뜻이었나? 그 헤더는 아마도 데이터 묶음에서 가장 최상단에 위치할 것이며, 또 몇자리를 쓰는지도 전부 정해져있겠지? 그러면 컴퓨터는 데이터가 들어왔을 때 앞에 n자리(약속된)를 먼저 확인해서 이 데이터를 unicode로 처리할지, rgb로 처리할지 등에 대해서 결정을 하고 갈 수 있겠지. 이것은 메모장 포맷과도 연결되는 것 같다. UTF가 뭔지는 모르겠는데 메모장 저장할 때 이러한 포맷이 일치하지 않았을 때, 남이 올려놨던 파일을 내 컴퓨터에서 열 경우 다이아몬드?들이 왕창 뜨는 것을 경험했다. 어떤 약속인지 컴퓨터는 스스로 그것을 판단할 수 없기에 정해진 포맷대로 해석하고 그것을 그대로 화면에 나타내는 거지. 

사진과 영상은 결국 똑같은 개념인데, 한 장이냐 연속의 차이일 뿐이니까 묶어서 봐도 되겠다. 수많은 픽셀이 모여 있으며, 그 픽셀들 하나하나에 정보를 포함하는 것을 0과 1로 처리하는 것이다. 그럼 '음'이라는 것은 어떻게 표현될까? 굳이 지금 당장 파고들 영역은 아닌 것 같다. 음을 표현하는 방식에 대해서 0과 1로 치환한 후 그것을 동일한 약속대로 다시 음으로 풀어서 스피커로 내보내는 과정이라는 것은 확실히 이해했기 때문이다. 

💡 Update 1: 추측에 대한 답과 유니코드 처리 방식에 대한 질문

하위 호환성(Backward Compatibility): 유니코드의 앞부분은 아스키코드와 동일하다. 그래서 옛날 컴퓨터와도 대화가 통한다

맥락(Context): 0과 1 그 자체는 의미가 없으며, 이걸 해석하는 맥락(헤더, 확장자, 포맷)이 있어야 비로소 정보가 된다.

추가적으로 든 의문은 유니코드에 대해서 설명할 때, 단순히 32bit라고 하지 않고, 8, 16, 24 or 32bit라고 설명했었다. 이는 byte 단위로 1,2,3,4byte라는 것인데, 컴퓨터는 기본적으로 '최대 효율'을 중요시 여기지 않은가? 그렇다면 ascii 코드로 충분한 내용은 헤더에 8bit(혹은 1byte)라는 정보를 담아서 보낼 것으로 추측할 수 있다. 반대로 그 패킷?에서 가장 큰 코드가 24bit를 포함해야 한다면, 헤더에 24bit라는 정보를 담아서 보낼 것 같은데.. 헤더에 그 정보가 담겨있지 않는다면 데이터 중간에 0과 1이 어떤 배치로 나와도 읽어들일 수가 없을 것이니 분명 헤더(가장 앞부분)에 몇 byte인지에 대한 정보가 담겨있을 것이다. 근데 여기서 궁금해지는 건, 극단적으로 1000개의 문자가 있는데 999개는 전부 ASCII고(=8bit로 표현 가능), 나머지 1개만 32bit를 전부 써야만 표현할 수 있는 데이터라고 하면, 이 패킷은 "32bit 데이터"라는 헤더를 포함하며 999개의 8bit 데이터 앞에 0을 24개를 더 붙여버리는 비효율적인 일을 반복할까? 아니면 헤더에 '몇번째 데이터만 32bit고 나머지는 8bit다'라는 데이터를 담아서 보낼까? 후자가 훨씬 효율적일 것이다. 근데 그러면 애매하게 여러가지 크기의 데이터가 섞인 케이스는 어떻게 처리할까? 그리고 하나의 패킷의 크기는 어떻게 될까? 여러개로 나뉘어서 보낼까? 위같은 특수한 상황에 대해서 과거의 설계자들은 어떠한 방식을 채택했을까?

위 내용에 대해서 Ai와 나누기 전에 스스로 조금만 더 생각해보자. 당장 생각나는 첫 번째 방식은 "크기 별로 보내는 거"다. 8bit부터 전부 보내는데, 특정 값을 (16bit, 24bit, 32bit의 자리)라는 값을 줘서, 그 자리는 공백으로 두고, 이후에 도착하는 16bit와 24bit, 32bit 패킷에서 그 자리를 채워넣는 방식이다. 물론 이게 가능할지는 모르겠다. 두 번째 방식은 그냥 "최대bit"에 맞춰서 앞에 0을 추가해버리는 거다. 최적화에 있어서는 상당히 구릴 것 같지만 제일 확실하지 않은가? 조금의 시간만 더 쓰면 될 수도 있지. 아마 작은 용량은 그냥 이대로 처리해버릴수도 있지 않을까? 세 번째 방식은 아까 생각했었던 좌표지정이다. 근데 이건 정말 극단적인 케이스에만 쓸모 있고, 그게 아니라면 헤더가 과도하게 길어질 것 같다. 네 번째는 "쪼개기"다. 패킷을 여러개로 쪼개는데, 단순히 몇비트마다 쪼개는게 아니라, 알고리즘을 이용해서 최대bit가 잘 분배되도록 쪼개는 거다. 앞부분은 대부분 8bit라면 8bit로 끝낼 수 있는 선에서 쪼개고, 그 이후에 24bit들이 듬성듬성 있다면 그것을 묶어서 쪼개고, 또 그 다음에 8bit가 반복된다면 그것을 쪼개는 방식이다. 솔직히 적고 보니 상당히 귀찮을 것 같다. 내가 개발자가 된다면 "정말 큰 대용량 파일"이 아닌 이상 최대bit에 맞춰서 보내는게 오히려 효율적일 것 같다는 생각이 들었다.

💡 Update 2: UTF, 신호등

내가 고민한 내용에 대한 해답은 가설 2와 4가 실제로 있었던 과거 선구자들의 고민이었다는 점을 배웠다.
- UTF-32 vs UTF-8
- UTF-32 (고정길이): 나의 가설2와 같다. 무조건 가장 큰 그릇에 담는다. 처리는 빠르지만 용량 남비가 극심할 확률이 높다.
- UTF-8 (가변길이) : 나의 가설4를 크게 발전시킨 형태다. 덩어리로 쪼개는 것을 넘어서 데이터의 첫번째 비트를 "신호수"로 두는 것이다. 그래서 한 byte가 0으로 시작하면 8bit로 보고, 1로 시작하면 그것을 기점으로 16,24,32bit인지 확인 후 처리하는 방식이다. 가장 최적화된 방식 같다.

여기서 1byte = 8bit인데, ASCII는 8bit라고 배웠으니까 0으로 시작한다고 한다면 -> 1로 시작하는 128-255까지의 값은 어떻게 되는거지? 라 생각했었지만, 원래 ASCII는 7bit라는 말을 들었다. 참으로 깔끔하게 떨어지는게 신기하다. (이후 확장 ASCII가 나왔지만, unicode 약속을 위해 폐기됐다)

그럼 시작bit에 1이 들어가면 16, 24, 32bit중 하나인데, 이것을 정확히 어떤 방식으로 처리할까?
byte단위로 묶어야 된다면, 1이 들어가는 순간 최소 2byte부터 4byte중 무엇인지 판단해야 할 것이다.
그럼 첫번째 bit는 1로 고정이니까 3가지 경우의 수 (16,24,32)를 담아내려면 앞 3개의 bit를 전부 써야겠지?
100 101 110 111 이렇게 4가지를 나타낼 수 있으니 말이다. 그럼 이 중 세개는 앞으로 나올 데이터는 16,24,32bit라는 약속일테고, 나머지 하나는 그냥 쓰지 않는 bit이거나, 어떠한 다른 목적으로 쓴다거나, 그 값이 나온다면 "데이터에 손상이 갔다"라는 신호가 될 수도 있겠네 (나올 수 없는 값일테니).

아무튼 그럼 3bit를 신호등으로 할당해야 되는데, 그럼 16bit는 사실상 13bit만, 24bit는 21bit만, 32bit는 29bit만 사용해서 다 표현하도록 했나? (앞의 3개의 bit가 변동되는 값은 약속하지 않은거지) 아마 그렇겠지

💡 Update 3: 놀라운 안전장치

내가 생각했던 방식과 유사하지만, bit수를 조금 더 희생함으로 "신뢰성"을 크게 올리는 방식을 채택했다.
나는 앞의 3bit를 사용한다 생각했지만, 실제로는 1이 몇 번 반복됐는가? (그 마침표는 0이다)로 설계됐다.
- 1byte : 0XXX XXXX
- 2byte : 110X XXXX 10XX XXXX
- 3byte : 1110 XXXX 10XX XXXX 10XX XXXX
- 4byte : 1111 0XXX 10XX XXXX 10XX XXXX 10XX XXXX

한 byte를 해석할 때 0부터 나오면 1byte로 처리한다.
1부터 나오면 1이 몇 번 나왔는지를 체크한다. 2byte는 110으로 시작한다. 1byte를 제외하고 최소 단위임에도 10이 아닌 이유는 연속되는 byte들에서 10으로 해당 byte가 열차에 달린 몸통임을 확인해줘야 했기 때문이다.
즉, 출발 열차에서 몇 byte인지 1의 개수로 판단한 다음, 다음 byte가 10으로 시작되면 연속된 열차임을 검증할 수 있다. 그리하여 머리에서 선언된 열차길이와 몸통(10으로 시작하는 byte)의 개수가 일치하지 않는다면, 컴퓨터는 그것이 잘못됐음을 검사하고 "대체 문자(Replacement Character)"로 표시해준 후에 넘어갈 수 있는거다.

그럼 각 byte는 얼마만큼의 bit를 희생할까?
[ 1byte - 1bit / 2byte - 5bit / 3byte 8bit / 4byte 11bit ]
굉장히 많이 희생하는 것 같지만 여전히 남은 자리는 많을 것 같다. 32bit에서 11bit를 희생해도 21bit인데, 이게 얼마나 큰 수인가? 모든 나라의 문자를 표현하고 이모티콘 등을 넣어도 부족함이 없어보인다.
--- 
### 📤 Output
**"No 약속, No 정보"**
