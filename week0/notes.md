CS50, notes
# 2진법
### 📥 Input
- 2진법
- 유일한 인풋, 전기 - 켜짐 꺼짐
- 트랜지스터 : 아주 작은 스위치 (물리적 정보 표현 값 저장) > 아주 조금의 전기를 저장해서 on하는, 나머진 끔으로 정보를 나타냄
- “정보를 표현할 때마다” 컴퓨터에서 일어나는 일
---
### 🧠 Process
2진법의 개념에 대해서는 이미 알고 있다. 우리에게 익숙한 10진법은 9에서 올림이 발생한다면, 2진법은 1에서 올림이 발생한다. 즉 한 자리에 경우의 수가 10개냐 2개냐의 차이일 뿐이다.
그렇다면 대체 왜? 컴퓨터는 2진수라는 틀에서 동작할까? 인간이 가진 개념처럼 10진수로 사용하면 안 됐나?
인체의 신비에 대해 다 알지 못 하나, 사람의 사고나 직관과 같은 것들을 해석하고 기술로 만들기란 쉽지 않나보다. 적어도 2진수 컴퓨터가 개발될 때는 그랬던 것 같다.
Ai는 어떤 방식으로 동작하는지 또 궁금해진다.
아무튼, 2진법으로 동작해야만 했던 이유에 대해 궁금해졌다. 또한 어떻게 보면 고작 2진법.. 아니 사실 10진법이라 해도 그걸 가지고 그처럼 다양한 작업들을 수행할 수 있음이 놀라울 따름이다.

왜 2진법인지에 대해 “인풋”으로 설명할 수 있을 것 같다. 모든 전자기기는 유일한 하나의 인풋을 가진다. 바로 “전기”, 우리는 전자기기를 동작하기 위해 충전을 한다.
충전이라 함은, 배터리에 전기를 채우거나 실시간으로 전기를 주는 과정이겠지. 2진법은 ‘수’의 영역이라고 하기엔 애매한 것 같다. 
2진법은 ‘수’보다는 “상태”를 나타낸다고 말하는게 더 정확할 것이다. 전기가 있냐 / 없냐가 1과 0인 것이다.
그렇기 때문에 컴퓨터는 2진법을 사용할 수밖에 없다. 10을 표현하기 위해선 전기가 꺼지는 것을 포함해 10가지 상태를 나타낼 수 있어야 하지 않은가?
굳이굳이 생각해보면 전기의 강약으로 하나의 트랜지스터에 여러 개의 값을 구현할 수 있으려나? 그러나 중요한 것은, 우리의 기술은 2진법으로 발전되어 왔고,
나는 컴퓨터를 잘 알고 활용하려는 사람이지, 새로운 컴퓨터 기술을 개발하려는 사람은 아니라는 점이다. 그렇기에 이 부분은 여기까지만.

결과적으로 컴퓨터는 “트랜지스터”라는 아주 작은 스위치를 통해 하나의 수, 혹은 bit를 가진다 할 수 있다.
이 트랜지스터가 한 컴퓨터에 무수히 많이 들어있으며, 이 트랜지스터가 켜져 있느냐(전류가 남아 있느냐) 꺼져 있느냐(전류가 남아있지 않느냐)를 통해서 정보를 나타내는 것이다.
솔직히 여전히 매우 놀랍기만 하다. 어찌보면 ‘고작?’이라고 여길 정도로 작은 트랜지스터의 전기 조합이 그러한 결과를 만들어낼 수 있다는 것이 정말로 신기하다.
앞으로 이 강의에서 그 과정을 상세하게 알아가길 기대해본다.

결국, “정보를 표현할 때마다” 컴퓨터에서는 무엇이 일어나는가? 그 정보를 표현하기 위해서 무수히 많은 트랜지스터가 onoff로 상태를 나타내는 것이다.
강의에서 123이라는 숫자를 보여주면서, 우리가 그것을 인식하는 것과 컴퓨터가 인식하는 것의 차이를 보여줬었다.
그럼 컴퓨터는 123이라는 숫자를 어떻게 화면에 띄우는 과정에까지 이르게 되는가? 123이라는 숫자를 2진수를 활용해 64+32+16+8+2+1, 즉 1111011로 나타낸다는 것 까진 알겠다. 
오케이, 123을 1111011로 저장했다는 것은 알겠다. 그럼 그걸 화면에 표시하는 과정은 또 어떻게 될까? 얼마나 치밀한 설계로 고작 “수”들의 조합으로 화면에 나타내는 명령이나,
색깔을 표현하거나 하는 등의 일이 가능할까? 이건 또 모니터 패널에 관한 영역인가? 패널을 생각해보니, 요즘 기술은 모르겠지만 옛날에 들었던 기억으로는
RGB, 즉 수많은 R, G, B 색을 내는 미세한 전구들이 배치되어 있고, 그것을 껏다 킴으로써 색 조합을 맞춰서 색을 낸다고 들었다. 
아마 그 전구는 “밝기”도 조절할 수 있을 것이다. 그러나 이 또한 자세히 파고 들면 2진법으로 밝기값을 저장하는 것이겠지?

컴퓨터를 파고드는 것은 상상 이상으로 즐거운 일이었구나.

💡 Update 1: 논리 회로에 대한 고찰

위에서 궁금한 내용에 관해서 Ai에게 질문을 했고, 몇 가지 유익한 대답을 들을 수 있었다.
전기의 강약을 통해 하나의 트랜지스터에 여러 개의 값을 구현하는 기술은 SSD에 활용되고 있다는 점이다. 신기하네.
그럼 이미 “가능”하다는 것이 밝혀졌는데 왜 컴퓨터는 2진수를 쓸까? 에 대해서도 “노이즈”라는 답변을 얻을 수 있었다.
10진법을 쓰려면 전기의 세기를 “나눠”야 하는데, 전기란 완전히 일정한 것이 아닌 출렁거리기 때문에, input과 output에 오차가 발생할 수 있다는 점이다.
그렇기 때문에 ‘정확’하게 하려면 최대한 그 변수를 통제해야 하고, 현재까지 최선의 방법이란 일정 값 이하는 전부 0, 그 이상은 전부 1로 처리하는 방식으로 정해졌다고 한다.
아마 그 “일정 값”에 근접할 일은 정말 거의 없겠지. 그 부분에 갔다면 오류가 발생하기 직전이었다는 말이니까.
갑자기 떠오른 생각은, 슈퍼마리오 시리즈 중에 한 게임의 스피드런 중 갑자기 y좌표가 심하게 바껴서 엄청난 단축이 일어났던 사건이다.
사람들은 이 글리치를 재현하고자 매우 많은 노력들로 수많은 시도를 했지만 한 번도 성공하지 못 했었다. 그러나 그 결과는 (원인은 기억이 안 나지만) 값이 변동되는… 무언가가 있었다고 했다.
오늘의 강의를 듣고, 추가적인 자료를 조사하면서 느낀 바는, 바로 이 노이즈의 오차가 “일정 값(1.5V?)”이라는 경계를 넘어버렸는데, 심지어 그것이 Y좌표에 영향을 줬던 것이라는 확신이 생겼다.
진짜 엄청난 운이었다고 밖에 말할 수가 없네..

또한, 트랜지스터 하나만 놓고 보면 단순히 onoff 스위치지만, 그룹으로 묶어 “논리 회로”를 만듦으로 AND, OR 연산 등을 수행하며 이걸 조합해서 CPU라는 계산기를 만들었다고 했다.
단순히 on/off 스위치일 때보단 뭘 조금 더 많이 할 수 있나? 싶지만 여전히 신기하고 대단하다.

예전에 회로에 대해 정말 겉핥기로 살짝 배웠었는데, 정말 여러가지가 있겠지만 대표적으로 AND, OR, XOR이 생각나네.
꼭 이게 아니더라도 on 과 off의 조합으로 무엇을 할 수 있는지를 생각해볼까.. 고작처럼 느껴지는 이 “회로”가 그 대단한 CPU를 만들어낸다니, 이건 이 강좌를 듣고 나서 생각해보지 않을수가 없었어.
…. 상당히 막막하다? and부터 하나씩 생각해보자. a와 b의 트랜지스터가 있다.. 그렇다면 and로 무엇을 표현할 수 있을까? “경우의 수”를 기준으로 생각해본다면, 2개의 트랜지스터는 4가지의 경우의 수를 만들 수 있다.
근데 그 두개를 묶어서 and를 한다면? true이거나, false이거나.. 고작 2가지다. 조합을 함으로 얻는 이득은 일단 “경우의 수”는 아니라는 것이다.
그럼 대체 and라는 연산은 무슨 이득이 있기에 사용되는건가? 컨트롤, 제어?라고 생각해볼 수 있을 것 같다. 단순히 나타낼 수 있는 경우의 수를 늘리는 것은 분명히 오류라 확신하니, 제어라 가정했을 때 어떻게 진행되는지 생각해보자.
“and는 원하는 값일 때만 1을 보낸다”라는 사실을 통해, a와 b가 둘 다 1일 때만 1을 넘기고, 그렇지 않는다면 0을 넘기게 된다. 결국 “논리 회로”란 이 과정을 더욱 효율적으로 하기 위한 수단일 것이다.
ab가 00일 때, 01일 때, 10일 때는 0이라는 값을 전달하고 싶다면, 논리회로가 없다면 3가지 경우는 0이다 라는 정보를 어딘가에 또 저장해야 될 것이고, 이것은 큰 자원 손실로 이어질 확률이 높을 것 같다.
당연히 ab가 11일 땐 1이다 라는 정보도 또한 저장해야겠지. 하지만 and회로를 설계한다면, 그러한 “저장매체” 혹은 ab를 “어떻게 처리할 것인지”에 대한 자료를 따로 저장할 필요가 없을 것이다.
이렇게 생각했을 때, 논리 회로에 대해서 이번 고찰로 내려볼 수 있는 가설이자 결론은 “논리회로가 없었어도 cpu는 똑같이 만들 수 있을 것 같다. 근데 훨씬 더 많은 트랜지스터가 필요했을 것이다”라는 점이다.

단순하게 결론 내리면 논리회로는 그냥 지름길인 것 같다. 논리회로가 없다고 못할 것은 없지만, 그 길이 너무도 오래 걸리는 방식이 될 테니까. and로만 생각해본 것이기에 다를 수도 있게지만,
모든 경우의 수에 대한 처리법을 저장한다고 한다면 결코 못할 것 같지는 않다 추측할 뿐이다.

💡 Update 2: Ai 피드백을 통한 논리회로에 대한 결론
논리 회로란 “제어”이며 스위치를 통해 ‘논리’를 구현한 것일 뿐이다. 흔히 컴퓨터가 계산을 하는 것, 수학을 하는 것처럼 보이는 것은 ‘논리’로 이루어진다.
---
### 📤 Output
**”컴퓨터는 계산기가 아니라, 논리가 구현된 ‘전기 통로’다”**
- **Why Binary? (Noise Tolerance):** 전압 차이를 이용해 더 큰 진법을 구사할 수 있겠지만, 전류는 오차가 존재하기 때문에, “신뢰성”을 최대한 지키기 위해 가장 확실한 2진법이 채택되었다.
- **What is Calculation? (Logic Gate):** 덧셈, 뺄셈 등은 적어도 컴퓨터에겐 그저 설계된 논리 게이트들이 전기 흐름을 ‘제어’한 결과일 뿐이다.
---
# 정보의 표현
### 📥 Input
- 숫자가 아닌 정보를 어떻게 표현?
- "약속"
- 'A' 는 65로 약속함
- ASCII라 하며, 8bit로 이루어짐
- 8bit로 부족해서 더 큰 단위까지 쓰는 유니코드 (상휘호환) 만듦
- 문자는 유니코드로 처리한다 하고, 사진 영상 음악 등은?
- RGB. 이미지의 1픽셀을 나타내는 기준이다
- 마찬가지로, 숫자에 따라 어떤 색을 나타낼지 '약속'함
- 72 73 33 -> R 72, G 73, B 33 > 도트 하나에 이와 같은 정보가 들어있어서 각 RGB의 정도에 따라 색이 나타남, 얼마만큼의 RGB를 각각 넣을 것인가? 를 숫자로 결정하기로 약속
- Digit의 어원은 손가락(10개)지만, CS에선 '정보를 표현하는 낱개 단위'를 뜻한다.
- 결국, 우리가 받는 모든 표현은 0과 1로 저장되어 약속에 의해 출력된 결과물일 뿐이다
---
### 🧠 Process
이번 강의에서 배운 내용은, 결국 "2진법" 강의에서 파악했던 내용들을 "실제로 어떻게 표현하는가?" 에 대한 실례를 알아본 시간이었다. 결국 컴퓨터는 0과 1밖에 할 수 없다. 그러나 이것을 통해 서로 "약속"을 하게 됨으로 우리가 보는 그 모든 과정이 이루어진다는 것이다. 우리가 서로 대화할 수 있는 것도 '언어'라는 약속이 있는 것처럼, 컴퓨터들도 어떠한 '약속'에 의해서 소통할 수 있다는 것이다. 단순히 같은 데이터(010001011010010101와 같은) 들을 주고 받을 수 있다 해도 그것을 해석할 수 없으면 고철일 뿐이지 않은가? 

그 약속은 ASCII Code부터 출발한다. 이 약속의 특징은 8bit를 사용한다는 것이다. 8bit는 10진법으로 256가지를 표현할 수 있겠지(0-255). 그러나 딱 봐도 이는 너무나 부족하다. 흔히 천자문이라 하는 한자만 담기에도 벅차며, 성조가 들어간 알파벳, 우리가 쓰는 한글 등은 약속의 방주에 탈 수가 없다. 그래서 상위호환으로 UniCode가 나왔다.
8, 16, 24bit 혹은 무려 32bit까지 정해진 약속이라는 점이다. 여기서 생긴 하나의 궁금점은 상위호환이라 했으니 ASCII의 8bit는 그대로 가져가면서, 추가적으로 약속해야 할 필요가 있는 문자들은 그 다음 수들에 배치했는가?이다. 정확히 이 내용에 대해 찾아본 적은 없지만 추측하건대, 아니 확신하건대 "맞다". 기존 약속을 완전히 깨부시고 새로운 체계를 도입했다면 굉장한 혼란이 있었을 것이며, 만약 그게 성공했다 치면, 지금 우리에게 굳이 "A"는 65다. 라는 정보를 가르칠 이유가 하등 없기 때문이다 (차라리 간략하게 이런 게 있었지만 지금은 유니코드로 바꼈다.. 정도로만 설명했을 것이다). 계속해서 A는 65, a는 97이라 가르치는 이유는 알파벳은 자주 쓰며, 유니코드로 바꼈어도 아스키의 영역은 그대로 보존되고 있었기 때문에 분명하게 동일할 것이다. 65, 97이라는 숫자는 잘 안 외워진다. 그냥 깔끔하게 16진수 41, 61이 훨씬 편하군.

Unicode에서 궁금한 건, 아니 아스키코드에서도 마찬가지로 궁금한 건.. 그래 아무튼 '약속'을 했다고 하자. 그럼 모든 데이터를 약속을 기준으로 받아들일 것인가? 72 73 33은 "Hi!"였다. 근데 이것을 이후에 나올 RGB에서는 노란색으로 하나의 픽셀을 나타낸다는 것을 배웠다. 그 차이는 어디에서 발생하는가? 컴퓨터는 0과 1을 받을 뿐이지, 이것이 Unicode 약속인지, RGB에 대한 약속인지는 알 수 없지 않는가? 그렇다면, 매 데이터를 보낼 때 이 데이터가 어떤 약속을 가지고 있는지에 대한 "헤더"가 들어갈 것이다. 예전에 배웠었던 헤더가 대충 이런 뜻이었나? 그 헤더는 아마도 데이터 묶음에서 가장 최상단에 위치할 것이며, 또 몇자리를 쓰는지도 전부 정해져있겠지? 그러면 컴퓨터는 데이터가 들어왔을 때 앞에 n자리(약속된)를 먼저 확인해서 이 데이터를 unicode로 처리할지, rgb로 처리할지 등에 대해서 결정을 하고 갈 수 있겠지. 이것은 메모장 포맷과도 연결되는 것 같다. UTF가 뭔지는 모르겠는데 메모장 저장할 때 이러한 포맷이 일치하지 않았을 때, 남이 올려놨던 파일을 내 컴퓨터에서 열 경우 다이아몬드?들이 왕창 뜨는 것을 경험했다. 어떤 약속인지 컴퓨터는 스스로 그것을 판단할 수 없기에 정해진 포맷대로 해석하고 그것을 그대로 화면에 나타내는 거지. 

사진과 영상은 결국 똑같은 개념인데, 한 장이냐 연속의 차이일 뿐이니까 묶어서 봐도 되겠다. 수많은 픽셀이 모여 있으며, 그 픽셀들 하나하나에 정보를 포함하는 것을 0과 1로 처리하는 것이다. 그럼 '음'이라는 것은 어떻게 표현될까? 굳이 지금 당장 파고들 영역은 아닌 것 같다. 음을 표현하는 방식에 대해서 0과 1로 치환한 후 그것을 동일한 약속대로 다시 음으로 풀어서 스피커로 내보내는 과정이라는 것은 확실히 이해했기 때문이다. 

💡 Update 1: 추측에 대한 답과 유니코드 처리 방식에 대한 질문

하위 호환성(Backward Compatibility): 유니코드의 앞부분은 아스키코드와 동일하다. 그래서 옛날 컴퓨터와도 대화가 통한다

맥락(Context): 0과 1 그 자체는 의미가 없으며, 이걸 해석하는 맥락(헤더, 확장자, 포맷)이 있어야 비로소 정보가 된다.

추가적으로 든 의문은 유니코드에 대해서 설명할 때, 단순히 32bit라고 하지 않고, 8, 16, 24 or 32bit라고 설명했었다. 이는 byte 단위로 1,2,3,4byte라는 것인데, 컴퓨터는 기본적으로 '최대 효율'을 중요시 여기지 않은가? 그렇다면 ascii 코드로 충분한 내용은 헤더에 8bit(혹은 1byte)라는 정보를 담아서 보낼 것으로 추측할 수 있다. 반대로 그 패킷?에서 가장 큰 코드가 24bit를 포함해야 한다면, 헤더에 24bit라는 정보를 담아서 보낼 것 같은데.. 헤더에 그 정보가 담겨있지 않는다면 데이터 중간에 0과 1이 어떤 배치로 나와도 읽어들일 수가 없을 것이니 분명 헤더(가장 앞부분)에 몇 byte인지에 대한 정보가 담겨있을 것이다. 근데 여기서 궁금해지는 건, 극단적으로 1000개의 문자가 있는데 999개는 전부 ASCII고(=8bit로 표현 가능), 나머지 1개만 32bit를 전부 써야만 표현할 수 있는 데이터라고 하면, 이 패킷은 "32bit 데이터"라는 헤더를 포함하며 999개의 8bit 데이터 앞에 0을 24개를 더 붙여버리는 비효율적인 일을 반복할까? 아니면 헤더에 '몇번째 데이터만 32bit고 나머지는 8bit다'라는 데이터를 담아서 보낼까? 후자가 훨씬 효율적일 것이다. 근데 그러면 애매하게 여러가지 크기의 데이터가 섞인 케이스는 어떻게 처리할까? 그리고 하나의 패킷의 크기는 어떻게 될까? 여러개로 나뉘어서 보낼까? 위같은 특수한 상황에 대해서 과거의 설계자들은 어떠한 방식을 채택했을까?

위 내용에 대해서 Ai와 나누기 전에 스스로 조금만 더 생각해보자. 당장 생각나는 첫 번째 방식은 "크기 별로 보내는 거"다. 8bit부터 전부 보내는데, 특정 값을 (16bit, 24bit, 32bit의 자리)라는 값을 줘서, 그 자리는 공백으로 두고, 이후에 도착하는 16bit와 24bit, 32bit 패킷에서 그 자리를 채워넣는 방식이다. 물론 이게 가능할지는 모르겠다. 두 번째 방식은 그냥 "최대bit"에 맞춰서 앞에 0을 추가해버리는 거다. 최적화에 있어서는 상당히 구릴 것 같지만 제일 확실하지 않은가? 조금의 시간만 더 쓰면 될 수도 있지. 아마 작은 용량은 그냥 이대로 처리해버릴수도 있지 않을까? 세 번째 방식은 아까 생각했었던 좌표지정이다. 근데 이건 정말 극단적인 케이스에만 쓸모 있고, 그게 아니라면 헤더가 과도하게 길어질 것 같다. 네 번째는 "쪼개기"다. 패킷을 여러개로 쪼개는데, 단순히 몇비트마다 쪼개는게 아니라, 알고리즘을 이용해서 최대bit가 잘 분배되도록 쪼개는 거다. 앞부분은 대부분 8bit라면 8bit로 끝낼 수 있는 선에서 쪼개고, 그 이후에 24bit들이 듬성듬성 있다면 그것을 묶어서 쪼개고, 또 그 다음에 8bit가 반복된다면 그것을 쪼개는 방식이다. 솔직히 적고 보니 상당히 귀찮을 것 같다. 내가 개발자가 된다면 "정말 큰 대용량 파일"이 아닌 이상 최대bit에 맞춰서 보내는게 오히려 효율적일 것 같다는 생각이 들었다.

💡 Update 2: UTF, 신호등

내가 고민한 내용에 대한 해답은 가설 2와 4가 실제로 있었던 과거 선구자들의 고민이었다는 점을 배웠다.
- UTF-32 vs UTF-8
- UTF-32 (고정길이): 나의 가설2와 같다. 무조건 가장 큰 그릇에 담는다. 처리는 빠르지만 용량 남비가 극심할 확률이 높다.
- UTF-8 (가변길이) : 나의 가설4를 크게 발전시킨 형태다. 덩어리로 쪼개는 것을 넘어서 데이터의 첫번째 비트를 "신호수"로 두는 것이다. 그래서 한 byte가 0으로 시작하면 8bit로 보고, 1로 시작하면 그것을 기점으로 16,24,32bit인지 확인 후 처리하는 방식이다. 가장 최적화된 방식 같다.

여기서 1byte = 8bit인데, ASCII는 8bit라고 배웠으니까 0으로 시작한다고 한다면 -> 1로 시작하는 128-255까지의 값은 어떻게 되는거지? 라 생각했었지만, 원래 ASCII는 7bit라는 말을 들었다. 참으로 깔끔하게 떨어지는게 신기하다. (이후 확장 ASCII가 나왔지만, unicode 약속을 위해 폐기됐다)

그럼 시작bit에 1이 들어가면 16, 24, 32bit중 하나인데, 이것을 정확히 어떤 방식으로 처리할까?
byte단위로 묶어야 된다면, 1이 들어가는 순간 최소 2byte부터 4byte중 무엇인지 판단해야 할 것이다.
그럼 첫번째 bit는 1로 고정이니까 3가지 경우의 수 (16,24,32)를 담아내려면 앞 3개의 bit를 전부 써야겠지?
100 101 110 111 이렇게 4가지를 나타낼 수 있으니 말이다. 그럼 이 중 세개는 앞으로 나올 데이터는 16,24,32bit라는 약속일테고, 나머지 하나는 그냥 쓰지 않는 bit이거나, 어떠한 다른 목적으로 쓴다거나, 그 값이 나온다면 "데이터에 손상이 갔다"라는 신호가 될 수도 있겠네 (나올 수 없는 값일테니).

아무튼 그럼 3bit를 신호등으로 할당해야 되는데, 그럼 16bit는 사실상 13bit만, 24bit는 21bit만, 32bit는 29bit만 사용해서 다 표현하도록 했나? (앞의 3개의 bit가 변동되는 값은 약속하지 않은거지) 아마 그렇겠지

💡 Update 3: 놀라운 안전장치

내가 생각했던 방식과 유사하지만, bit수를 조금 더 희생함으로 "신뢰성"을 크게 올리는 방식을 채택했다.
나는 앞의 3bit를 사용한다 생각했지만, 실제로는 1이 몇 번 반복됐는가? (그 마침표는 0이다)로 설계됐다.
- 1byte : 0XXX XXXX
- 2byte : 110X XXXX 10XX XXXX
- 3byte : 1110 XXXX 10XX XXXX 10XX XXXX
- 4byte : 1111 0XXX 10XX XXXX 10XX XXXX 10XX XXXX

한 byte를 해석할 때 0부터 나오면 1byte로 처리한다.
1부터 나오면 1이 몇 번 나왔는지를 체크한다. 2byte는 110으로 시작한다. 1byte를 제외하고 최소 단위임에도 10이 아닌 이유는 연속되는 byte들에서 10으로 해당 byte가 열차에 달린 몸통임을 확인해줘야 했기 때문이다.
즉, 출발 열차에서 몇 byte인지 1의 개수로 판단한 다음, 다음 byte가 10으로 시작되면 연속된 열차임을 검증할 수 있다. 그리하여 머리에서 선언된 열차길이와 몸통(10으로 시작하는 byte)의 개수가 일치하지 않는다면, 컴퓨터는 그것이 잘못됐음을 검사하고 "대체 문자(Replacement Character)"로 표시해준 후에 넘어갈 수 있는거다.

그럼 각 byte는 얼마만큼의 bit를 희생할까?
[ 1byte - 1bit / 2byte - 5bit / 3byte 8bit / 4byte 11bit ]
굉장히 많이 희생하는 것 같지만 여전히 남은 자리는 많을 것 같다. 32bit에서 11bit를 희생해도 21bit인데, 이게 얼마나 큰 수인가? 모든 나라의 문자를 표현하고 이모티콘 등을 넣어도 부족함이 없어보인다.
--- 
### 📤 Output
**"No 약속, No 정보"**
---
# 알고리즘
### 📥 Input
- 문제 해결을 위한 단계적 방법일 뿐
- 대부분의 경우, 우리가 당연히 하는 직관이나 생각들을 기계가 이해하도록 번역하는 것
- 그럼 뭐가 좋은 알고리즘인가?
- 의사 코드(pseudo code)
--- 
### 🧠 Process
알고리즘은 참으로 많이 들어봤다. 이 단어는 단순히 컴퓨터에서 쓰이는 것을 넘어서, 요즘 아이들은 "유튜브" 혹은 "인스타그램" 등에 더 연관지어서 생각할 것이다. 결국 그 또한 '알고리즘'이겠지.
알고리즘이란, '처리'라고 생각하면 될 것 같다. 강의에서도 Input > [ Black Box ] > Output을 초반에 보여주고, 지금 와서 "이것은 알고리즘이었다"고 설명한다.
나는 앞서서 컴퓨터는 0과 1로 이루어져 있을 뿐임을 알게 되었다. 그렇다면, 이 "처리과정"인 알고리즘, 단순하게 생각하면 "논리의 흐름"이라고도 볼 수 있는 이 알고리즘은 0과 1에서 어떠한 영향을 끼치게 될 것인가? 

위 내용에 대한 설명을 하긴 어려울 것 같다. 강의에서 설명한 알고리즘은, 그냥 "문제 해결 방식 매뉴얼"이었지만, 내가 설명하고자 하는 것은 그 메뉴얼을 0과 1이 어떻게 처리하는지?에 대한 고민이기 때문이다. 교수가 이번 강의에서 소개한 알고리즘은 2가지다. 하나는 순차적으로 페이지를 확인하며 찾는 방법, 다른 하나는 절반씩 쪼개가는 방법이다.

천 명의 이름이 담긴 책에서 "Sky Ha"라는 이름을 찾기 위해서 1페이지부터 하나씩 본다고 가정했을 때, 그것을 0과 1로 대체 어떻게 구현할 수 있을까? 일단 여기에서 한 가지 전제가 있었다. "가나다 순"으로 이미 깔끔하게 정렬되어 있었다는 것이다. 이 정렬 또한 알고리즘의 힘이겠지만, 지금 당장은 이 부분은 해결되어 있다고 치자. "Sky Ha"라는 데이터를 분석해보면, unicode를 통해 S, k, y, ' ', H, a 로 되어 있겠지. 이 책은 이미 가나다 순으로 정렬이 돼있으니까, 가장 앞에 S만 보면 될 거다. S는 0x53이며, 컴퓨터가 맨 앞에서부터 "이름"이라는 단위를 하나씩 검사할 거다. 가장 앞에 코드가 0x53보다 작으면, 다음으로 넘기고 같으면 'k'를 검증하겠지. 그리고 또 0x53으로 시작하는 모든 값을 뒤졌음에도 'Sky Ha'가 발견되지 않았다면, 책에 없음을 파악하고 종료하는 정도의 안전장치는 둘 수 있을 것이다. 그럼 대체 어떻게 0x53인지 검증할까? 우리가 보기엔 그냥 if(첫글자 = 0x53) 정도로 간단하게 생각하겠지만, 나는 0과 1이 이 일을 어떻게 수행하는지가 궁금하다.

아마 이것을 제어하기 위해선, '논리회로'가 필요하지 않을까?
0x53을 2진수로 표현하면, 0101 0011 이다. 그럼 컴퓨터는 첫 데이터를 읽고, 0101 0011을 tmp와 같은 곳에 저장해둔 다음에, 각 bit에 대해서 xor 연산을 진행하겠지? (and나 or은 절대 안 되고, 아무튼 두 값이 일치하냐 / 다르냐를 판단할 수 있는 회로를 쓸 것이 분명하다. 그게 xor이 맞는지는 잘 모르겠네). 그럼 또 '주소'도 필요하겠네. 책의 첫 이름부터 끝 이름까지 주소가 있을테고, 그것 또한 가지고 있으면서, 한 이름이 찾는 이름이 아닐 때 주소값을 +1 하는 과정이 추가되겠지, 그럼 순서 상으로는 정답이 아닐 땐 n번 주소 xor 연산 > n++ 을 반복하게 될 거다. 
아마 이렇게 돌아갈 수밖에 없을 것이다. 근데 이렇게 '돌아가는 게' 나는 신기하다. 지금 이것을 고민해서 답이 나올까? 0과 1을 다루는 것에 있어서 "이해"는 했지만, 실제로 그것이 동작하는 과정이 상당히 궁금하게 느껴지는 하루다.

결국 컴퓨터는 하등 존재이며, 사람은 고등 존재인 것 같다. 아무리 컴퓨터가 좋은 기술로 만들어지고 놀라운 회로들의 조합이라 하여도, 사람이 훨씬 차원이 높은 존재다. 생명의 탄생의 신비부터, 성장에 필요한 것들이 주어지며 사람은 커간다. 그리하여 사람에게는 '당연히 할 수 있는 것들'이 매우 많다. 주소록에서 찾는 거? 컴퓨터처럼 열심히 코딩을 안 해도 적당히 머리가 큰 사람이라면 알아서 'S는 어딨냐..'부터 시작해서 찾아내겠지. 알고리즘은 이렇게 사람이 당연히 할 수 있는 것들을 아주 세세하게 쪼개서 지시하는 것이라 할 수 있겠다. 그러나 때로는 0과 1로 이루어진 무수한 트랜지스터들이.. 마치 개미와 같이 하나하나의 힘은 매우 약할지언정 사람에겐 사실상 불가능한, 혹은 사람보다 훨씬 빠른 속도로 수많은 일들을 처리할 수 있다는 것 또한 사실일 것이다.

이러한 알고리즘에서 무엇이 좋은지는 딱 2가지 측면으로 정리되지 않을까? 하나는 "신뢰성"이며, "효율성"일 것 같다. 결과가 신뢰도가 있어야 하며, 탐색하는데 걸리는 자원(시간, 혹은 메모리 할당량 등)이 적을 수록 좋다. 당장 생각해본다면, 신뢰도가 무조건 100%여야 하지 않나? 라고 생각이 들었지만, 그것 어디까지나 case by case 일 것 같다. 신뢰도를 조금 포기하고 엄청난 자원 절약을 할 수 있다면, 그것이 무조건 나쁘다고만은 할 수 없을 것 같다. 예를 들어 "책에 무조건 찾고자 하는 이름이 있다!" 라는 전제조건이 붙어있는 상황에서 "확실하게 한 번 만에 찾는 100시간 짜리 알고리즘" vs "10%로 찾을 수 있지만 10초가 걸리는 알고리즘" 이 있다면, 그냥 후자를 찾을 때까지 반복하는게 훨씬 이득이겠지.
아무튼 모든 상황에서 신뢰도를 100% 기준치로 잡고 생각할 필요는 없는 것 같다.

이러한 알고리즘을 0과 1로 풀어서 설명할 수 있겠지만, 그것은 사람에게 적합하지 않다. 사람은 차원이 다른 '직관'을 가졌기 때문에, "흐름"만 설명해도 이해할 수 있다. 컴퓨터는 그것을 이해하지 못 하지만 말이다. 그래서 사용하는게 "의사코드"다. 이것은 컴퓨터에게 알려주기 위해서 만드는게 아닌, 사람 입장에서 이해를 돕기 위한 작업이며, 사람은 의사코드를 통해 알고리즘을 이해하고, 이것을 컴퓨터에게 수준을 낮춰서 설명해줘야만 작업이 완료됐다고 할 수 있다. 컴퓨터가 어떻게 동작하는지 깊게 이해하며 탐구하는 것도 중요하지만, 의사코드를 잘 활용해서 사람인 내가 훨씬 빠르게 논리를 구현하는 것 또한 열심히 연습해야겠다.

💡 Update 1: null, 주소 +1의 숨은 의미, Big O Notation

문자열의 끝에는 무조건 \0 (Null Terminator)가 숨겨져 있다고 한다. 왜냐하면 메모리란, 빈집을 주는게 아니라 전 세입자가 있었던 상태며, Sky Ha가 문자열로 들어와 있다 해도, 컴퓨터가 읽을 때 'a' 뒤의 문자까지 읽어버리기 때문에, 그것을 끝낸다는 약속으로 0x00인 null을 넣는다. 만약 이 약속이 null이 아닌 K였다 치면 컴퓨터는 데이터를 이렇게 읽겠지. Sky HaKSky HbcKSli.... 그럼 구분하기 힘들테니... Null의 소중함을 다시 한 번 기억하게 됐다. 

또, 주소값을 +1하는 과정은 '포인터 연산'과 관련이 있다고 한다. 그러나 이 +1이 무조건 1byte가 아님을 기억하고 있어야 되는 것 같다. Sky Ha만 봐도 null을 포함해 S, k, y, ' ', H, a, \0 총 7개의 byte를 사용하고 있지 않은가? 만약 +1을 단순하게 해버린다면 BlueSky Ha라는 이름을 컴퓨터는 "옳다 요놈이구나!" 해버릴 듯 싶다. (B > l > u > e까지는 틀렸다고 여기다가 S부터 찾아버리니까 말이다). 이렇게 보니 Ctrl + F 기능을 사용해서 "포함된"을 찾을 때는, 주소를 +1 Byte로 찾을 것 같다고 생각이 든다. BlueSky Ha라는 단어가 있다면, 이 또한 'Sky Ha'를 찾을 때 검색해야 되니 말이다. 반대로 정확히 일치한 것을 찾을 때는 무조건 첫 글자부터 확인하고 아니면 바로 다음 byte가 아닌 다음 "이름"으로 넘어가는 과정이 있겠지. 그래서 "데이터 타입"을 알고 있어야 주소를 얼마나 점프할 지 계산할 수 있다고 했다.

음.. 근데, data 타입이라는 정보에 Sky Ha가 총 7개의 byte라는 정보가 있을까? 많은 사람들의 이름이 기록되어 있고 글자 수는, 즉 값의 크기는 엄청 다양할 것이다. 근데 data type으로 판단할 수가 있나? int일 때 4byte를 점프한다, char일 때 1byte를 점프한다 까지는 알겠는데, 이름이 다 다른데 어떻게 정확히 다음 이름으로 점프할 수 있을까? 제일 편한 방법은 최대 글자 수에 맞춰 모든 이름을 그만큼의 크기로 할당해버리면 되겠지. 그럼 점프할 때 그냥 n byte씩 점프하면 그만일테지. 근데 그런 비효율을 썼을리는 없을거다. 아마 '가변 string?' 같은게 들어가서 몇 byte를 점프해야 되는지 쉽게 찾아낼 수 있겠지? 그 원리에 대해서 지금 고민하는 건 조금 과한 것 같다. 나중에 data type에서 배우고 고민해보자.

결과적으로 그냥 이름 길이를 정해버리는 방식과, 자유롭게 주소로 쓰는 방식을 둘 다 쓴다는 것을 알게 됐다. 전자는 고민할 것도 없고, 후자는 "주소"를 적극 활용한다는 것을 알게 됐다. 주소값은 모두 동일한 크기를 가진다. "점프"는 이름이 아닌 주소를 점프하는 거다. A라는 이름을 보고 아니네? 그럼 A의 위치에서 점프하는게 아니라 A의 주소를 점프하면, 무조건 B의 주소가 나오는거다. 그럼 B의 주소를 찾아가는 방식이다. 이런 식이라면, 데이터의 양에 따라 결정할 수 있겠다. 한국으로 치면 이름이 대부분 3글자다. 아무리 커도 4글자라 쳤을 때, 4글자는 소수니까 특수케이스로 분류하고, 3글자 고정으로 쓰면 "주소"를 할당할 자원을 아낄 수 있다. 2글자 이름은 한 자리 조금 버려도 사소하다. 반대로 엄청 들쭉날쭉한 영어 이름같은 경우, 차라리 주소를 할당하는게 훨씬 자원을 아낄 것이다. 두 방식을 결정하는데 유일한 시금석은 '효율'이겠지.

알고리즘의 효율성을 "값"으로 나타내는 방법이 있다고 한다. 바로 Big O다. 대충 O(x)와 같은 방식으로 쓰는 것 같다. x에는 "최악의 경우의 수"를 적어야 하는 것 같다. '이 알고리즘에서 가장 오래 걸렸을 때가 몇 번인가?'를 적으면 O(x)가 얼마나 뛰어난 알고리즘인지 비교할 수 있는 표기법이다. 순차 탐색은 N명의 사람을 찾을 때 운 없으면 가장 끝자리, 즉 N번이니까 O(N)이다. 만약 바보같이 한 이름을 두 번씩 보는 알고리즘이면 O(2N)이겠고, 강의에서 나왔던 것처럼 2명씩 넘기는 알고리즘은 운이 없어서 2명씩 넘기는 그 사이에 걸려서 "어? Sky Ha를 넘어갔네?"라고 보고 다시 앞으로 한 단계를 간다 가정해도 O(1/2N+1)정도겠지. 아마 +1은 정말 사소하니까 생략하지 않을까 싶다. 그리고 이진 탐색은 O(log N)이었다. 강의시간에 표로 보여준 내용이었는데, 빅 오로 이렇게 표현하구나. 아무튼 x값을 봄으로 어떤 알고리즘이 "더 우월한가"에 대해 쉽게 판단할 수 있는 표기법이라는 것을 알게 되었다.
더 알아보니 상수나 계수는 그냥 버린다고 한다. +1 뿐만 아니라 2N도 그냥 N이랑 비슷하다 이거다. 알고리즘은 "정말 많은 데이터"를 처리하는 것을 기준으로 보기 때문에, 2배 차이는 결국 2배밖에 안 난다는 거다. 단순히 log N과 비교해봐도 엄청난 차이가 나니 납득할만하다.
---
### 📤 Output
**"개발자는 사장이다. 직원들을 어떻게 일 시키느냐에 따라 회사의 가치가 결정된다"**
---
# 스크래치: 기초
### 📥 Input
- scratch.mit.edu
---
### 🧠 Process
조금이나마 코딩을 해본 입장에서 이번 강의는 딱히 피드백할게 없다.
그냥 쉽고 간단하게, 스크래치의 기초 사용법과 Input > 알고리즘(function) > Output에 대해 알아봤다. 스크래치는 그것을 눈으로 쉽게 볼 수 있게 설계된 초심자용 프로그래밍 언어다
---
### 📤 Output
Null
---
# 스크래치: 심화
### 📥 Input
- broadcast
- 추상화, 직접 함수 만들기 (어떻게 동작하는지는 중요치 않음, 동작하면 그만임)
---
### 🧠 Process
이번 강의에서는, 훨씬 더 심화된 버전의 스크래쳐들을 볼 수 있었다. 그 원리가 무엇인지 다 이해가 된다. 처음에 broadcast를 통해 event를 호출하면, 그 이벤트를 받아서 또 다른 동작을 실행하는 모습이 있었다. 근데 그 event는 당장 스크래쳐 메인 화면에 보이지 않았어서, 신기했었지만.. 결국 이후에 나온 '추상화'와 같은 개념인 것 같았다. broadcast(event) 를 하면, 'When I receive(event)' 가 동작하는 것이다. 처음엔 '함수'와 다른게 뭐지? 라 생각했었는데, 이벤트를 받는 리시브는 노란 블럭임을 발견했다. 노란 블럭은 event라는 카테고리로 나와있으며, 함수는 contol이라는 주황 블럭인가... 라고 생각했었지만, 아닌 것 같다.

함수는 그냥 모든 기능이 함수라 할 수 있지 않을까? 흔히 프로그래밍 할 때, 변수를 선언하는 것도, int main(void)와 같은 선언문을 여는 것도, if문도, 상수를 선언하는 것도.. 전부 함수일 것이다. 그럼 Event는 함수와 다른 종류가 아닌, 함수 중 하나인 것으로 이해해도 되지 않을까? 결국 스크래쳐 블록의 색깔들은 '사람'을 위한 것이지, 컴퓨터를 위함이 아니다. 컴퓨터는 전부 함수(더 깊게 들어가면 0과 1로) 이루어져 있을 뿐이겠지.

아무튼, 이 강의에서 진행한 broadcast(event)와 함수의 차이는 내 생각엔 이렇다. broadcast는 말 그대로 뿌린다. 즉, 방송국에서 방송을 뿌리면, 그것을 수신하는 "모든 TV"에서 실행하게 된다. 반대로 함수는 1:1 통신일 것이다. 함수가 동작하는 순간, 컴퓨터는 해당 함수가 존재하는 파일로 가서 그 코드를 '순차적'으로 읽을 것이다. 우리가 디버그할 때 보면 무조건 코드를 한 줄씩 읽지 않는가? if, break 등은 그 읽는 순서를 제어할 뿐이고. 그렇기 때문에 완전 같다고 할 수는 없겠지만 (물론 broadcast의 동작도 함수처럼 설계되어 있지 않을까?).. 

여기서 한 가지 강사의 주장, 즉 강사가 말하는 "나아가야 할 방향"과 내가 생각하는 방향이 다른 점을 발견했다. 강사는 함수에 대해서, 추상화에 대해서 언급하면서 이것을 "동작 원리는 중요하지 않아, 호출해서 동작하면 잘 된거다"라는 발언을 했다. 이것이 진정으로 '동작 원리'를 낮춰 말하는 뜻은 아님이 확실하다. 그 말 뜻은, 프로그래밍을 하는 입장에서 if문이 어떻게 하나하나 동작하는지를 굳이 파고 들 필요가 없다는 말일 것이다. 왜냐하면, 이미 동작 원리를 설계해서 아주 잘 만들어진 함수니까 말이다. 그러니까 개발자는 그냥 '사용'하면 된다. 하지만 또한 이번 강의에서 '직접 함수를 만드는' 것을 보여줬다. 이는 결코 동작 원리를 무시해선 안 된다는 말이다. 잘 만들어진 함수만을 이용해서 프로그래밍 하기엔 아쉬운 부분들이 많다. 그렇기 때문에 나에게 맞는 최적의 함수를 직접 설계해야 될 필요가 수준이 올라갈수록 필연적으로 생기게 될 것이다(혹은 잘 만들어진 함수를 가져와서 그것을 내 쓸모에 맞게 변형하는 일을 한다 해도 말이다). 결국 이러한 과정에선 동작원리를 이해하지 못하는 개발자는 힘들어질 것이다.

아무튼, 함수는 추상화라는 말은 이러한 맥락에서 "잘 만들어놨으면 어렵게 분석할 필요 없이 가져다 쓰면 되는 편리한 도구"라는 정의를 내릴 수 있을 것 같다. 잘 만들기 위해선, 또 잘 만들어진 것을 조금씩 개조하기 위해선 '분석'할 필요가 있겠지만, 당장 가져다 쓰는 입장에서는 중요도가 떨어지는 것은 분명하다.

그렇다면, 나는 해커가 되고 싶은데.. 아직 해킹이라는 것이 어떻게 동작하는지는 모르겠다. 해커로서 내가 "코드가 동작하는 과정"을 공격하는 것이 가능할지도 모르겠다. 내가 생각하는 해킹은 그냥 데이터를 가로채거나 하는 등의 영역이였지, 개발코드와 직접적으로 뭔가를 하는 것을 본 기억이 별로 없기 때문이다 (나의 견문은 좁다). 그러나 분명히 최정상의 해커로 나아가려면 이러한 '당연한 함수'들을 파해치고 분석해서 취약점을 찾아낼 수만 있다면, 그곳으로 나는 뛰어들어야만 할 것 같다. 그래서 강사가 말하는 뉘앙스인 "동작하기만 하면 된다"와 내가 생각하는 앞으로의 방향인 "잘 동작하는 물건의 빈틈을 찾아라"는 살짝 다른 결이 될 것 같다. 개발자는 그렇게까지 하진 않아도 충분하겠지만 말이다.

💡 Update 1: 모든 것이 함수는 아니며, 정확히 구분할 줄 알아야 함
- 함수 (Function): "Precedural (절차적)"
- Broadcast (Event): "Event-Driven (이벤트 기반) / Observer Pattern"

- 함수(Function): Call 명령어 > "저기 100번지 주소로 가서 실행하고 다시 와". (메모리 주소 이동 O)"
- 제어문(if, while): Branch / Jump 명령어 > "값이 0이면 3줄 밑으로 점프해." (그냥 흐름만 바꿈, 함수처럼 복귀 주소를 저장하거나 판을 벌리지 않음)
- 변수 선언 (int a): 명령어가 아님 > "메모리 땅따먹기" (동작이 아니라 공간 확보)

이렇게 세 가지가 컴퓨터 입장에서 다르게 돌아간다는 것을 알고 있어야 한다. 함수는 메모리 주소가 왕복이고, 제어문은 그냥 편도다. 변수 선언은 메모리를 확보하기만 하는 거다. 왕복과 편도는 이해 됐는데, 변수 선언은 뭘까? 결국 메모리를 확보하고 '다시 돌아오는' 개념이 아닌가? 그럼 함수랑 다를게 뭐지?

또한 이것이 해킹에서 중요한 이유는, 해커는 '변수(데이터)인 줄 알았던 것에 함수(코드)를 넣어서 실행시키는 공격을 한다(Shellcode Injection). 그렇기 때문에 데이터와 코드를 구분하는 엄격한 기준이 세워져 있어야 한다고 한다. 아까 함수는 '왕복'이라 했는데, 해커는 '돌아오는 표'를 조작해서 컴퓨터를 납치할 수 있다고 한다.

그리고 개발자는 추상화를 믿고, 해커는 그 또한 의심하는 자라 했는데 실제로 Log4j 사태가 있었다. 개발자들은 그 함수를 그냥 믿고 썼지만, 해커들이 동작 원리를 파해쳐서 전 세계가 뒤집어지는 취약점이 발견된 것이다.

당연한 것을 파해치는 것, 옳다고 생각하던 것들의 헛점을 끊임없이 의심해보는 것, 그것이 내가 가야 할 길이라면 상당히 마음에 든다. 그리고 내가 생각했던 해킹과 다르게 코드가 동작하는 해킹을 공격하는 것이 "시스템 해킹"이라고 하는 것을 알게 됐다. Flow Hijacking은 if(비밀번호 일치) > 로그인 이라는 코드에서 if문을 강제로 넘겨버릴 때 '로그인'이 동작해버리는 취약점이고, Buffer Overflow는 돌아갈 주소를 바꿔치기 하는 기술이라고 하는 것 또한 알게 됐다. 

💡 Update 2: 함수와 변수가 '같은 왕복 여행'이 아닌 이유
- 함수 (왕복 여행) > 서울(Main)에서 일하다가 부산(Function)에 출장 다녀와
- 동작
1. 현재 위치(서울) 저장. (Return Adderess)
2. 부산으로 점프 (CALL)
3. 일이 마치면 저장해준 서울 주소를 보고 복귀 (RET)
- 핵심: **제어권(Control)이 다른 곳으로 갔다가 온다

- 변수 선언 (영토 확장) > 서울 내 책상(Stack)이 너무 좁네? 옆으로 좀 밀어서 공간을 확보해
- 동작
1. 어디 안 감, 점프도 안 함
2. 그냥 **스택 포인터(Stack Pointer)**라는 막대기를 아래로 쓱 내림 (SUB ESP)
3. 이제 여기부터 4칸은 내 땅(int a) 이다. 끝
- 핵심: 제어권이 이동하지 않음. 그냥 앉은 자리에서 공간만 넓히는 행위

결국 함수 호출은 제어권이 위아래로 움직이는데 반해, 변수 선언은 제어권이 움직이지 않고 공간만 차지한다고 한다고 한다. '제어권'이라는 건 무엇일까? 컴퓨터 입장에선 '제어권'이 어떠한 권리라기 보단, 결국 전부 0과 1로 동작할 것이다. 그럼 '제어권'이라는게 실존한다면, 사람이 컴퓨터 동작을 구분하기 위해 만든 걸까? 마치 여러 명이 둘러쌓여 있을 때, 발언권이 마이크를 가진 사람에게만 있다면, 마이크가 제어권인 느낌인가? 결국 0과 1로 이루어져 있다면, 개발자 입장에서 변수를 선언할 때 제어권 이동이 없으니 안전하다고 느껴도, 변수 자리에 "제어권을 가진 코드"을 넣어버리면 컴퓨터는 그 코드를 통해 동작해버리니까 해커 입장에서는 '제어권'을 복제할수도 있고, 강탈할 수도 있고, 소멸시킬 수도 있을까? 결국 해커인 내가 변수, 함수 등을 "명확히 구분 할 수 있어야 함은" 그것이 그 한계 내에서 움직이기 때문이 아니라, 그러한 한계를 기준으로 제작되어 있는 코드기 때문이고, 나는 그 제한에 속박될 필요는 없을 것 같다.

💡 Update 3: 제어권의 실체 "RIP 레지스터"

내가 생각했던 허상 같던 제어권의 실체를 알게 됐다. 컴퓨터 안에 수많은 저장 공간이 있지만, CPU가 유일하게 복종하는 단 하나의 왕이 있다는 것이다.
- 이름: PC (Program Counter) 또는 IP (Instruction Pointer). (64bit 컴퓨터에선 RIP로 불린다). 
- 역할: "다음에 실행할 코드의 주소(번지수)"를 적어두는 곳
결국 제어권이란, RIP 레지스터를 뜻하며, 이 자리에 뭐가 적혀있느냐?가 컴퓨터의 다음 목적지라는 것이다. 즉 RIP 레지스터를 건들 수만 있다면 급격하게 제어권이 넘어오기 쉽다. CPU는 그냥 RIP가 가리키는 곳으로 가서 거기에 있는 0과 1을 실행한다. 그것이 무슨 코드인지는 판단할 능력이 없다. 
제어권을 뺏긴다는 것은 프로그램의 동작이 완전히 무너져버릴 수 있다는 것을 의미하는 것이라는 점을 배웠다. 제어권을 뺏는 방법은 여러 개가 있지만 유명한 방법으론 스택 버퍼 오버플로우가 있다. 변수가 저장된 위치가 100번지고, 돌아갈 주소가 200번지에 있다면, 변수(4칸 차지)에 데이터를 200번지까지 넘치게 때려부어서 200번지 값이 기존 리턴 주소가 아닌 해커의 의도한 주소로 옮겨버리는 방법이라 한다. 변수에 들어갈 값이 초과되면 막아버리는 방지책 하나만 있어도 막힐테지만, 그럼 또 다른 방법으로 뚫고자 하는 것이 해커니까 계속 반복되겠지.
---
### 📤 Output
**"당연한 것을 당연하지 않게 한 번 더 고민해보는 마음가짐을 가져야 한다. 컴퓨터는 0과 1이지만, 그 속에서 권력을 지고 있는 RIP 레지스터라는 제어권의 존재를 인식했다"**